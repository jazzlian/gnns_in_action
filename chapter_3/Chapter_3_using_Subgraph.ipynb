{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 3: Convolutional GNNs: GCN and GraphSage\n",
        "\n",
        "Introduction\n",
        "This notebook demonstrates the use of Convolutional Graph Neural Networks (GCNs) and GraphSAGE for graph representation learning, leveraging the PyTorch Geometric library. To ensure compatibility with resource-limited environments, we utilize a subgraph containing only 10,000 nodes, which is less than 1% of the total nodes in the OGBN-Products dataset. The key processes covered include setting up the environment, data preparation, model definition, training, and evaluation."
      ],
      "metadata": {
        "id": "RKbII5TU0i2C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1: Environment Setup\n",
        "\n",
        "In this section, we ensure the environment is correctly configured with the necessary libraries and dependencies. We start by checking the CUDA version and PyTorch installation, followed by installing additional required libraries."
      ],
      "metadata": {
        "id": "v8jIZdkk0udO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qGNOTVXJe0s",
        "outputId": "ccd35e8a-0d44-473c-f4b3-48527627a417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.1\n"
          ]
        }
      ],
      "source": [
        "# Find the CUDA version PyTorch was installed with\n",
        "!python -c \"import torch; print(torch.version.cuda)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPtDuwKQJhrd",
        "outputId": "12c03688-a943-4131-bb5d-37830c3525c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu121\n"
          ]
        }
      ],
      "source": [
        "# PyTorch version\n",
        "!python -c \"import torch; print(torch.__version__)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvfhHBSP-CU0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use the above information to fill in the http address below\n",
        "%%capture\n",
        "!pip install ogb pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.3.0+cu121.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn5s_l2rJnHB"
      },
      "outputs": [],
      "source": [
        "# import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import NeighborSampler\n",
        "from torch_geometric.nn import SAGEConv, GCNConv\n",
        "from torch_geometric import utils, loader\n",
        "from torch_geometric.utils import subgraph\n",
        "\n",
        "from torch_geometric.nn import (\n",
        "    Aggregation,\n",
        "    MaxAggregation,\n",
        "    MeanAggregation,\n",
        "    MultiAggregation,\n",
        "    SoftmaxAggregation,\n",
        "    StdAggregation,\n",
        "    SumAggregation,\n",
        "    VarAggregation,\n",
        "    LSTMAggregation\n",
        ")\n",
        "\n",
        "# importing obg datatset\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
        "\n",
        "from pandas.core.common import flatten\n",
        "# import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# sns.set(rc={'figure.figsize':(16.7,8.27)})\n",
        "# sns.set_theme(style=\"ticks\")\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "import collections\n",
        "import os.path as osp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "# from pandas.core.common import flatten\n",
        "# from scipy.special import softmax\n",
        "\n",
        "import random\n",
        "\n",
        "# Setting the seed\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "# torch.backends.cudnn.deterministic = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 2: Loading the Graph Data and Creating a Subgraph\n",
        "In this section, we load the OGBN-Products dataset and create a subgraph consisting of 10,000 nodes. This subset of the full dataset allows us to perform experiments in environments with limited computational resources while still demonstrating the effectiveness of graph convolutional networks (GCNs) and GraphSAGE.\n",
        "\n",
        "#### Step 1: Load the OGBN-Products Dataset\n",
        "First, we will load the OGBN-Products dataset using the PyTorch Geometric library. The dataset contains product information from Amazon, where nodes represent products and edges represent co-purchasing relationships.\n",
        "\n",
        "\n",
        "#### Step 2: Create a Subgraph\n",
        "To create a subgraph with 10,000 nodes, we randomly select a subset of nodes from the full graph. This subset will be used for training and evaluating our models.\n",
        "\n",
        "\n",
        "#### Step 3: Process Mappings for Labels and Product IDs\n",
        "Next, we load and process mappings for label indices to product categories and product IDs. These mappings will help us interpret the results and visualize the data.\n",
        "\n",
        "\n",
        "#### Step 4: Verify the Subgraph\n",
        "Finally, we verify that the subgraph has been created correctly by checking the number of nodes and edges.\n",
        "\n",
        "\n",
        "By completing these steps, we have successfully loaded the OGBN-Products dataset and created a subgraph with 10,000 nodes. This subgraph will be used in subsequent sections to train and evaluate our GCN and GraphSAGE models."
      ],
      "metadata": {
        "id": "RAdScP311Q7w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4wp2QR237-q"
      },
      "outputs": [],
      "source": [
        "opt = 'GCN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsNnQsl6JuUo"
      },
      "outputs": [],
      "source": [
        "# Load the OGB evaluator for the dataset\n",
        "evaluator = Evaluator(name='ogbn-products')\n",
        "\n",
        "# Establish the device for model training 'cuda' if GPU, 'cpu' otherwise\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device)\n",
        "\n",
        "# Confirm the device. If it's a GPU, 'cuda' will print\n",
        "print('Device: {}'.format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPvfDWUHJsHW"
      },
      "outputs": [],
      "source": [
        "# root = osp.join(osp.dirname(osp.realpath('./')), 'data', 'products')\n",
        "# download and loading the obg dataset\n",
        "root = osp.join(osp.dirname(osp.realpath('./')), 'content')\n",
        "if opt == 'GCN':\n",
        "  dataset = PygNodePropPredDataset( name='ogbn-products', root=root)\n",
        "else:\n",
        "  dataset = PygNodePropPredDataset( name='ogbn-products', transform=T.ToSparseTensor(), root=root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIu96ENEJ7OM"
      },
      "outputs": [],
      "source": [
        "data = dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu7iHMfOU3up"
      },
      "outputs": [],
      "source": [
        "# Define the indices of nodes to include in your subset\n",
        "subset_indices = torch.arange(0, 10000)  # For example, first 10,000 nodes\n",
        "\n",
        "\n",
        "# Extract the subgraph corresponding to the subset\n",
        "subset_edge_index, edge_attr, edge_mask = subgraph(subset_indices, data.edge_index, None, relabel_nodes=True, num_nodes=data.num_nodes, return_edge_mask=True)\n",
        "\n",
        "\n",
        "# Adjust node features and labels for the subset\n",
        "subset_features = data.x[subset_indices]\n",
        "subset_labels = data.y[subset_indices]\n",
        "\n",
        "# Create a new graph object for the subset\n",
        "subset_graph = data.__class__()\n",
        "subset_graph.edge_index = subset_edge_index\n",
        "subset_graph.x = subset_features\n",
        "subset_graph.y = subset_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dg05XxyEDysI"
      },
      "outputs": [],
      "source": [
        "# Load the mapping from label indices to product categories\n",
        "path_to_file = '/content/ogbn_products/mapping/labelidx2productcategory.csv.gz'  # Adjust the path as needed\n",
        "df = pd.read_csv(path_to_file)\n",
        "\n",
        "# Create a dictionary mapping label indices to product categories\n",
        "index_product_dict = dict(zip(df['label idx'], df['product category']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52Huqzv3F1XM"
      },
      "outputs": [],
      "source": [
        "# Load the mapping from label indices to product IDs\n",
        "path_to_asin_file = '/content/ogbn_products/mapping/nodeidx2asin.csv.gz'  # Adjust the path as needed\n",
        "asin_df = pd.read_csv(path_to_asin_file)\n",
        "\n",
        "# Create a dictionary mapping label indices to product categories\n",
        "node_asin_dict = dict(zip(asin_df['node idx'], asin_df['asin']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR3xuJdAGZqO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Verify the subgraph\n",
        "print(f\"Number of nodes in the subgraph: {subset_graph.num_nodes}\")\n",
        "print(f\"Number of edges in the subgraph: {subset_graph.num_edges}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIiItbceXomj"
      },
      "outputs": [],
      "source": [
        "subset_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 3: Model Definition, Training, and Evaluation\n",
        "In this section, we define and train our first GCN and GraphSAGE models. We will define the architecture of both models, set up the training process, and evaluate their performance. Additionally, we will use category and product/ASIN information to evaluate the accuracy of the category predictions and to generate product recommendations using similarity.\n",
        "\n",
        "-\n",
        "#### Step 1: Define the GCN Model\n",
        "We start by defining the GCN model. The model consists of two graph convolutional layers (GCNConv), each followed by a ReLU activation function. The output layer applies a log softmax function to generate class probabilities.\n",
        "\n",
        "-\n",
        "#### Step 2: Define the GraphSAGE Model\n",
        "Next, we define the GraphSAGE model. Similar to the GCN model, it consists of two graph convolutional layers (SAGEConv), each followed by a ReLU activation function. The output layer applies a log softmax function to generate class probabilities.\n",
        "\n",
        "-\n",
        "\n",
        "#### Step 3: Define the Training and Evaluation Process\n",
        "We set up the optimizer, loss function, and training loop for both models. The training process involves forward propagation, loss computation, backpropagation, and optimization. We also implement functions to evaluate the model's performance on the validation set.\n",
        "\n",
        "-\n",
        "#### Step 4: Evaluate the Models\n",
        "After training, we evaluate the performance of the GCN and GraphSAGE models on the test set. We calculate metrics such as F1 score and log loss to assess the quality of the predictions.\n",
        "\n",
        "-\n",
        "\n",
        "#### Step 5: Visualize the Training and Validation Loss\n",
        "Finally, we plot the training and validation loss curves for both models to visualize their learning progress over epochs.\n",
        "\n",
        "-\n",
        "\n",
        "#### Step 6: Use Category and Product/ASIN Information\n",
        "We utilize the category and product/ASIN information to further analyze the model's performance and generate recommendations:\n",
        "\n",
        "We analyze the accuracy of category predictions by comparing the true labels with the predicted labels. This helps us understand the model's performance in predicting product categories.\n",
        "Generate Product Recommendations:\n",
        "\n",
        "Using the learned embeddings from the models, we compute the cosine similarity between products to generate recommendations. This involves finding the most similar products to a given product based on their embeddings.\n",
        "\n"
      ],
      "metadata": {
        "id": "NetsC4tS2JCq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_OZsyTRXokF"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, return_embeds=False):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        if return_embeds:\n",
        "            return x\n",
        "        return torch.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "gcn_model = GCN(in_channels=dataset.num_features, hidden_channels=64, out_channels=dataset.num_classes)\n",
        "\n",
        "\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, return_embeds=False):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        if return_embeds:\n",
        "            return x\n",
        "        return torch.log_softmax(x, dim=1)\n",
        "\n",
        "graphsage_model = GraphSAGE(in_channels=dataset.num_features, hidden_channels=64, out_channels=dataset.num_classes)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train():\n",
        "    gcn_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = gcn_model(data.x, data.edge_index)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-1b17mpaME9"
      },
      "outputs": [],
      "source": [
        "subset_graph.num_features = subset_graph.x.size(1)\n",
        "\n",
        "\n",
        "# Ensure 'y' is a tensor of class labels\n",
        "subset_graph.y = subset_graph.y.squeeze()  # Remove any extra dimensions\n",
        "\n",
        "# If 'y' contains integer class labels\n",
        "if subset_graph.y.dim() == 1:\n",
        "    num_classes = int(subset_graph.y.max().item()) + 1\n",
        "\n",
        "# If 'y' contains one-hot encoded labels\n",
        "elif subset_graph.y.dim() == 2:\n",
        "    num_classes = subset_graph.y.size(1)\n",
        "\n",
        "subset_graph.num_classes = num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vc5cKWRaPc-"
      },
      "outputs": [],
      "source": [
        "num_classes, subset_graph.num_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxFbNfMEboye"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_split_masks(num_nodes, train_frac=0.6, val_frac=0.2, test_frac=0.2):\n",
        "    \"\"\"\n",
        "    Generates masks for train, validation, and test splits.\n",
        "\n",
        "    Args:\n",
        "        num_nodes (int): The number of nodes in the dataset.\n",
        "        train_frac (float): The fraction of nodes to include in the training set.\n",
        "        val_frac (float): The fraction of nodes to include in the validation set.\n",
        "        test_frac (float): The fraction of nodes to include in the test set.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing boolean masks for training, validation, and test splits.\n",
        "    \"\"\"\n",
        "    # Ensure fractions sum up to 1\n",
        "    assert train_frac + val_frac + test_frac == 1, \"Fractions must sum up to 1.\"\n",
        "\n",
        "    # Generate a random permutation of node indices\n",
        "    indices = torch.randperm(num_nodes)\n",
        "\n",
        "    # Determine split sizes\n",
        "    train_size = int(num_nodes * train_frac)\n",
        "    val_size = int(num_nodes * val_frac)\n",
        "\n",
        "    # Create masks\n",
        "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "    # Assign masks based on split sizes\n",
        "    train_mask[indices[:train_size]] = True\n",
        "    val_mask[indices[train_size:train_size + val_size]] = True\n",
        "    test_mask[indices[train_size + val_size:]] = True\n",
        "\n",
        "    return {'train_mask': train_mask, 'val_mask': val_mask, 'test_mask': test_mask}\n",
        "\n",
        "# Usage example with a subset graph\n",
        "subset_masks = generate_split_masks(num_nodes=subset_graph.num_nodes)\n",
        "\n",
        "# Applying the masks to the subset graph\n",
        "subset_graph.train_mask = subset_masks['train_mask']\n",
        "subset_graph.val_mask = subset_masks['val_mask']\n",
        "subset_graph.test_mask = subset_masks['test_mask']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icfjRz2bXohW"
      },
      "outputs": [],
      "source": [
        "# Update the in_channels and out_channels according to your subset_graph features and classes\n",
        "gcn_model = GCN(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=subset_graph.num_classes)\n",
        "graphsage_model = GraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=subset_graph.num_classes)\n",
        "\n",
        "optimizer_gcn = torch.optim.Adam(gcn_model.parameters(), lr=0.01)\n",
        "optimizer_sage = torch.optim.Adam(graphsage_model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize variables to track the best model\n",
        "best_val_loss_gcn = float('inf')\n",
        "best_model_state_gcn = None\n",
        "\n",
        "best_val_loss_sage = float('inf')\n",
        "best_model_state_sage = None\n",
        "\n",
        "def train(model, optimizer, data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask].squeeze())  # Ensure the target tensor is of the correct shape\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be9YErOVdYOi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Initialize lists to store the training and validation loss values for each epoch\n",
        "train_loss_gcn = []\n",
        "val_loss_gcn = []\n",
        "train_loss_sage = []\n",
        "val_loss_sage = []\n",
        "\n",
        "def validate(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x, data.edge_index)\n",
        "        val_loss = criterion(out[data.val_mask], data.y[data.val_mask].squeeze())  # Make sure the target tensor is of the correct shape\n",
        "    return val_loss.item()\n",
        "\n",
        "for epoch in range(200):\n",
        "    loss_gcn = train(gcn_model, optimizer_gcn, subset_graph)\n",
        "    train_loss_gcn.append(loss_gcn)\n",
        "    current_val_loss_gcn = validate(gcn_model, subset_graph)\n",
        "    val_loss_gcn.append(current_val_loss_gcn)\n",
        "\n",
        "    # Check if the current validation loss is the best we've seen, save model if it is\n",
        "    if current_val_loss_gcn < best_val_loss_gcn:\n",
        "        best_val_loss_gcn = current_val_loss_gcn\n",
        "        best_model_state_gcn = gcn_model.state_dict()\n",
        "\n",
        "    loss_sage = train(graphsage_model, optimizer_sage, subset_graph)\n",
        "    train_loss_sage.append(loss_sage)\n",
        "    current_val_loss_sage = validate(graphsage_model, subset_graph)\n",
        "    val_loss_sage.append(current_val_loss_sage)\n",
        "\n",
        "    # Similarly for GraphSAGE\n",
        "    if current_val_loss_sage < best_val_loss_sage:\n",
        "        best_val_loss_sage = current_val_loss_sage\n",
        "        best_model_state_sage = graphsage_model.state_dict()\n",
        "\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch}, GCN Loss: {loss_gcn:.4f}, GraphSAGE Loss: {loss_sage:.4f}, GCN Val Loss: {val_loss_gcn[-1]:.4f}, GraphSAGE Val Loss: {val_loss_sage[-1]:.4f}')\n",
        "\n",
        "\n",
        "# Optionally, load the best model state back into the model\n",
        "gcn_model.load_state_dict(best_model_state_gcn)\n",
        "graphsage_model.load_state_dict(best_model_state_sage)\n",
        "\n",
        "\n",
        "def test_model(model, data):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x.to(device), data.edge_index.to(device))  # Forward pass\n",
        "        preds = torch.argmax(out, dim=1)[data.test_mask]  # Only take the predictions from the test mask\n",
        "        true_labels = data.y[data.test_mask]  # True labels for the test data\n",
        "    return preds.cpu().numpy(), true_labels.cpu().numpy()\n",
        "\n",
        "# Plotting the training and validation loss curves for GCN\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_loss_gcn, label='GCN Training Loss')\n",
        "plt.plot(val_loss_gcn, label='GCN Validation Loss', linestyle='--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('GCN Training and Validation Loss Curves')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the training and validation loss curves for GraphSAGE\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_loss_sage, label='GraphSAGE Training Loss')\n",
        "plt.plot(val_loss_sage, label='GraphSAGE Validation Loss', linestyle='--')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('GraphSAGE Training and Validation Loss Curves')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxquFKKWcZJI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def evaluate(model, data, mask):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        logits = model(data.x, data.edge_index)  # Forward pass\n",
        "        preds = logits.argmax(dim=1)  # Get the predicted classes\n",
        "\n",
        "        # Calculate F1 score\n",
        "        f1 = f1_score(data.y[mask].cpu().numpy(), preds[mask].cpu().numpy(), average='weighted')\n",
        "\n",
        "        # Calculate log loss using PyTorch's NLLLoss\n",
        "        log_probs = F.log_softmax(logits[mask], dim=1)\n",
        "        loss_function = torch.nn.NLLLoss()\n",
        "        logloss = loss_function(log_probs, data.y[mask])\n",
        "\n",
        "    return f1, logloss.item()\n",
        "\n",
        "# Example usage\n",
        "f1_gcn, logloss_gcn = evaluate(gcn_model, subset_graph, subset_graph.val_mask)  # Evaluate GCN on validation set\n",
        "f1_sage, logloss_sage = evaluate(graphsage_model, subset_graph, subset_graph.val_mask)  # Evaluate GraphSAGE on validation set\n",
        "\n",
        "print(f\"GCN F1 Score: {f1_gcn:.4f}, Log Loss: {logloss_gcn:.4f}\")\n",
        "print(f\"GraphSAGE F1 Score: {f1_sage:.4f}, Log Loss: {logloss_sage:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHyiN3TyDhRH"
      },
      "outputs": [],
      "source": [
        "def analyze_category_predictions(true_labels, pred_labels):\n",
        "    analysis_results = {}\n",
        "    for category in set(true_labels):\n",
        "        indices = [i for i, label in enumerate(true_labels) if label == category]\n",
        "\n",
        "        if not indices:  # Skip categories not present in the subset\n",
        "            continue\n",
        "\n",
        "        category_preds = [pred_labels[i] for i in indices]\n",
        "        correct_preds_count = category_preds.count(category)\n",
        "\n",
        "        mispredictions = [pred for pred in category_preds if pred != category]\n",
        "        most_common_misprediction, most_common_misprediction_count = Counter(mispredictions).most_common(1)[0] if mispredictions else (\"None\", 0)\n",
        "\n",
        "        analysis_results[category] = {\n",
        "            'Correct Prediction Percentage': correct_preds_count / len(indices) * 100 if indices else 0,\n",
        "            'Correct Prediction Count': correct_preds_count,\n",
        "            'Most Common Misprediction': most_common_misprediction,\n",
        "            'Misprediction Percentage': most_common_misprediction_count / len(indices) * 100 if mispredictions and indices else 0,\n",
        "            'Misprediction Count': most_common_misprediction_count,\n",
        "            'Total Count': len(indices)\n",
        "        }\n",
        "\n",
        "    return analysis_results\n",
        "\n",
        "all_true_labels = [index_product_dict[label.item()] for label in subset_graph.y]\n",
        "\n",
        "with torch.no_grad():\n",
        "    gcn_all_logits = gcn_model(subset_graph.x, subset_graph.edge_index)\n",
        "    gcn_all_preds = torch.argmax(gcn_all_logits, dim=1)\n",
        "    gcn_all_preds = [index_product_dict[pred.item()] for pred in gcn_all_preds]\n",
        "\n",
        "    sage_all_logits = graphsage_model(subset_graph.x, subset_graph.edge_index)\n",
        "    sage_all_preds = torch.argmax(sage_all_logits, dim=1)\n",
        "    sage_all_preds = [index_product_dict[pred.item()] for pred in sage_all_preds]\n",
        "\n",
        "gcn_analysis_all = analyze_category_predictions(all_true_labels, gcn_all_preds)\n",
        "\n",
        "# Example: Printing analysis results for the GCN model for all nodes\n",
        "for category, stats in gcn_analysis_all.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    print(f\"GCN Correct Predictions: {stats['Correct Prediction Count']} / {stats['Total Count']} ({stats['Correct Prediction Percentage']:.2f}%)\")\n",
        "    print(f\"GCN Most Common Misprediction: {stats['Most Common Misprediction']} - {stats['Misprediction Count']} / {stats['Total Count']} ({stats['Misprediction Percentage']:.2f}%)\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTLikiTBDhUy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the analysis results into a list of dictionaries for DataFrame construction\n",
        "data = []\n",
        "for category, stats in gcn_analysis_all.items():\n",
        "    data.append({\n",
        "        'Category': category,\n",
        "        'GCN Correct Predictions (%)': f\"{stats['Correct Prediction Percentage']:.2f}% ({stats['Correct Prediction Count']}/{stats['Total Count']})\",\n",
        "        'GCN Most Common Misprediction': f\"{stats['Most Common Misprediction']} ({stats['Misprediction Percentage']:.2f}% of {stats['Total Count']})\",\n",
        "        'Total Count': stats['Total Count']  # Adding the total count\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the list of dictionaries\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Sort the DataFrame by the 'Total Count' column in descending order\n",
        "df.sort_values('Total Count', ascending=False, inplace=True)\n",
        "\n",
        "# Optional: drop the 'Total Count' column if you don't want to display it\n",
        "# df = df.drop(columns=['Total Count'])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0aE3xavivyb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5X9dH54DhXc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Analyze the predictions made by the GraphSAGE model\n",
        "sage_analysis_all = analyze_category_predictions(all_true_labels, sage_all_preds)\n",
        "\n",
        "# Convert the analysis results into a list of dictionaries for DataFrame construction\n",
        "sage_data = []\n",
        "for category, stats in sage_analysis_all.items():\n",
        "    sage_data.append({\n",
        "        'Category': category,\n",
        "        'GraphSAGE Correct Predictions (%)': f\"{stats['Correct Prediction Percentage']:.2f}% ({stats['Correct Prediction Count']}/{stats['Total Count']})\",\n",
        "        'GraphSAGE Most Common Misprediction': f\"{stats['Most Common Misprediction']} ({stats['Misprediction Percentage']:.2f}% of {stats['Total Count']})\",\n",
        "        'Total Count': stats['Total Count']  # Adding the total count\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the list of dictionaries\n",
        "sage_df = pd.DataFrame(sage_data)\n",
        "\n",
        "# Sort the DataFrame by the 'Total Count' column in descending order\n",
        "sage_df.sort_values('Total Count', ascending=False, inplace=True)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(sage_df)\n",
        "sage_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "106FnB7Wvq24"
      },
      "outputs": [],
      "source": [
        "pd.concat([df, sage_df.drop(columns=['Category'])], axis=1).drop(columns=['Total Count'])\n",
        "\n",
        "\n",
        "# df.columns = ['Product Category'] + df.columns[1:].tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPOqjb2cvqz7"
      },
      "outputs": [],
      "source": [
        "# Load trained models (assuming they are already trained and loaded into gcn_model and graphsage_model)\n",
        "gcn_model.eval()\n",
        "graphsage_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    gcn_embeddings = gcn_model(subset_graph.x, subset_graph.edge_index, return_embeds=True)\n",
        "    graphsage_embeddings = graphsage_model(subset_graph.x, subset_graph.edge_index, return_embeds=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wOa9yGnvqwQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compute cosine similarity matrices\n",
        "gcn_similarity_matrix = cosine_similarity(gcn_embeddings.cpu().numpy())\n",
        "graphsage_similarity_matrix = cosine_similarity(graphsage_embeddings.cpu().numpy())\n",
        "\n",
        "# Example usage: Find the top-k most similar items to a given product node\n",
        "product_idx = 123  # Replace with the index of the product node\n",
        "top_k = 6\n",
        "\n",
        "# For GCN embeddings\n",
        "top_k_similar_indices_gcn = np.argsort(-gcn_similarity_matrix[product_idx])[:top_k]\n",
        "print(f\"Top {top_k} similar products to product {product_idx} according to GCN:\")\n",
        "print(top_k_similar_indices_gcn)\n",
        "\n",
        "# For GraphSAGE embeddings\n",
        "top_k_similar_indices_sage = np.argsort(-graphsage_similarity_matrix[product_idx])[:top_k]\n",
        "print(f\"Top {top_k} similar products to product {product_idx} according to GraphSAGE:\")\n",
        "print(top_k_similar_indices_sage)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGEhgBGT6eMz"
      },
      "outputs": [],
      "source": [
        "node_asin_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0ETp6gB6eJr"
      },
      "outputs": [],
      "source": [
        "list(top_k_similar_indices_gcn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9IXlRFV6eG1"
      },
      "outputs": [],
      "source": [
        "[ node_asin_dict[x] for x in list(top_k_similar_indices_gcn) ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x5aOyC8I7F0"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "6\" Handheld Brass Telescope with Wooden Box - Pirate Navigation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOqvmgU36d2T"
      },
      "outputs": [],
      "source": [
        "# Funko POP Television: Adventure Time Marceline Vinyl Figure\n",
        "\n",
        "\n",
        "# Adventure Time 5\" Finn with Accessories\n",
        "\n",
        "# Funko My Little Pony: DJ Pon-3 Vinyl Figure\n",
        "\n",
        "# My Little Pony: Twilight Sparkle\n",
        "\n",
        "\n",
        "# Plastic Gold Coins 288ct With 24 Pirate Themed tatoos\n",
        "\n",
        "# Handheld Brass Telescope with Wooden Box - Pirate Navigation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNNSGgwwIuO9"
      },
      "outputs": [],
      "source": [
        "list(top_k_similar_indices_sage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD1ltQEBIuL7"
      },
      "outputs": [],
      "source": [
        "[ node_asin_dict[x] for x in list(top_k_similar_indices_sage) ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 4: Experimenting with Aggregations\n",
        "In this section, we experiment with different aggregation functions within the GraphSAGE model to understand their impact on model performance. Aggregation functions determine how node features are combined from neighboring nodes, which can significantly affect the learning process. We will test mean, sum, and max aggregation functions and evaluate their effects on training and validation performance.\n",
        "\n",
        "#### Step 1: Define the GraphSAGE Model with Custom Aggregation\n",
        "We modify the GraphSAGE model to accept an aggregation function as a parameter. This allows us to easily switch between different aggregation strategies and observe their effects on the model's performance.\n",
        "\n",
        "#### Step 2: Train and Evaluate the Model with Different Aggregations\n",
        "We define a function to train and evaluate the GraphSAGE model with a specified aggregation function. This function trains the model, computes the loss, and evaluates the validation accuracy at regular intervals. By comparing these metrics, we can determine the effectiveness of each aggregation strategy.\n",
        "\n",
        "#### Step 3: Training Curve Comparison\n",
        "To visualize the impact of different aggregation functions, we plot the training and validation loss curves for each aggregation strategy (mean, sum, max). This helps us understand how each aggregation function influences the convergence and performance of the model over time.\n",
        "\n",
        "#### Step 4: Analyze Category Predictions\n",
        "We further analyze the model's predictions by evaluating the accuracy of category predictions. We assess the accuracy of the model's category predictions by comparing the predicted labels with the true labels. This helps us understand the model's effectiveness in classifying products into their respective categories."
      ],
      "metadata": {
        "id": "poW3pRVQ7TVY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dihsu1jgDhZ3"
      },
      "outputs": [],
      "source": [
        "# Section 3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nGOMemvDhch"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, agg_func='mean'):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.agg_func = agg_func\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels, self.agg_func)\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels, self.agg_func)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return torch.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwTI_91SDhfJ"
      },
      "outputs": [],
      "source": [
        "def train_evaluate(agg_func):\n",
        "    model = GraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, agg_func=agg_func).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(100):  # You can adjust the number of epochs\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(subset_graph.x.to(device), subset_graph.edge_index.to(device))\n",
        "        loss = criterion(out[subset_graph.train_mask], subset_graph.y[subset_graph.train_mask].squeeze().to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                preds = out.argmax(dim=1)\n",
        "                correct = (preds[subset_graph.val_mask] == subset_graph.y[subset_graph.val_mask].squeeze().to(device)).sum()\n",
        "                acc = int(correct) / int(subset_graph.val_mask.sum())\n",
        "                print(f'Epoch: {epoch}, Loss: {loss.item():.4f}, Val Acc: {acc:.4f}')\n",
        "\n",
        "    return acc\n",
        "\n",
        "# Aggregation functions to test\n",
        "aggregations = ['mean', 'sum', 'max']\n",
        "\n",
        "# Evaluate models with different aggregation functions\n",
        "for agg in aggregations:\n",
        "    print(f'\\nTesting with {agg} aggregation:')\n",
        "    acc = train_evaluate(agg)\n",
        "    print(f'Validation Accuracy with {agg} aggregation: {acc:.4f}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxh1dutcDhkZ"
      },
      "outputs": [],
      "source": [
        "# Training curve comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzp4YvY_Dhm7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_model(model, data, optimizer, criterion):\n",
        "    model.train()  # Set the model to training mode\n",
        "    optimizer.zero_grad()  # Clear gradients\n",
        "\n",
        "    out = model(data.x.to(device), data.edge_index.to(device))  # Forward pass\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask].squeeze().to(device))  # Calculate loss only on the training mask\n",
        "    loss.backward()  # Backpropagate the loss\n",
        "    optimizer.step()  # Update model parameters\n",
        "\n",
        "    return loss.item()  # Return the loss value\n",
        "\n",
        "# Assuming GraphSAGE model is defined with an 'agg_func' parameter to specify the aggregation function\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, agg_func='mean'):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr=agg_func)  # First GraphSAGE layer with specified aggregation\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr=agg_func)  # Second GraphSAGE layer with specified aggregation\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Create models with different aggregation functions\n",
        "model_mean = GraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, agg_func='mean').to(device)\n",
        "model_sum = GraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, agg_func='add').to(device)  # Use 'add' for sum aggregation\n",
        "model_max = GraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, agg_func='max').to(device)\n",
        "\n",
        "# Create separate optimizers for each model\n",
        "optimizer_mean = torch.optim.Adam(model_mean.parameters(), lr=0.01)\n",
        "optimizer_sum = torch.optim.Adam(model_sum.parameters(), lr=0.01)\n",
        "optimizer_max = torch.optim.Adam(model_max.parameters(), lr=0.01)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loops for different aggregation strategies\n",
        "losses_mean_agg, losses_sum_agg, losses_max_agg = [], [], []\n",
        "\n",
        "for epoch in range(1000):\n",
        "    loss_mean = train_model(model_mean, subset_graph, optimizer_mean, criterion)\n",
        "    losses_mean_agg.append(loss_mean)\n",
        "\n",
        "    loss_sum = train_model(model_sum, subset_graph, optimizer_sum, criterion)\n",
        "    losses_sum_agg.append(loss_sum)\n",
        "\n",
        "    loss_max = train_model(model_max, subset_graph, optimizer_max, criterion)\n",
        "    losses_max_agg.append(loss_max)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Mean Agg Loss: {loss_mean:.4f}, Sum Agg Loss: {loss_sum:.4f}, Max Agg Loss: {loss_max:.4f}\")\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses_mean_agg, label='Mean Aggregation')\n",
        "plt.plot(losses_sum_agg, label='Sum Aggregation')\n",
        "plt.plot(losses_max_agg, label='Max Aggregation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.title('Training Loss Curves for Different Aggregation Strategies')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Dt0L4GgDhp0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import sort_edge_index, subgraph\n",
        "\n",
        "\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, agg_func='mean'):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr=agg_func)  # First GraphSAGE layer with specified aggregation\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr=agg_func)  # Second GraphSAGE layer with specified aggregation\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "def evaluate_model(model, data, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x.to(device), data.edge_index.to(device))  # Forward pass\n",
        "        val_loss = criterion(out[data.val_mask], data.y[data.val_mask].squeeze().to(device))  # Calculate loss on the validation mask\n",
        "    return val_loss.item()\n",
        "\n",
        "# Training and evaluation loop\n",
        "def train_and_evaluate(model, data, optimizer, criterion, epochs):\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_model(model, data, optimizer, criterion)\n",
        "        val_loss = evaluate_model(model, data, criterion)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "model_mean = GraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, agg_func='mean').to(device)\n",
        "model_sum = GraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, agg_func='add').to(device)  # Use 'add' for sum aggregation\n",
        "model_max = GraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, agg_func='max').to(device)\n",
        "\n",
        "\n",
        "# Create separate optimizers for each model\n",
        "optimizer_mean = torch.optim.Adam(model_mean.parameters(), lr=0.01)\n",
        "optimizer_sum = torch.optim.Adam(model_sum.parameters(), lr=0.01)\n",
        "optimizer_max = torch.optim.Adam(model_max.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate the models\n",
        "train_losses_mean, val_losses_mean = train_and_evaluate(model_mean, subset_graph, optimizer_mean, criterion, 200)\n",
        "train_losses_sum, val_losses_sum = train_and_evaluate(model_sum, subset_graph, optimizer_sum, criterion, 200)\n",
        "train_losses_max, val_losses_max = train_and_evaluate(model_max, subset_graph, optimizer_max, criterion, 200)\n",
        "\n",
        "\n",
        "\n",
        "# Calculate global minimum and maximum\n",
        "global_min = min(min(train_losses_mean), min(val_losses_mean), min(train_losses_sum), min(val_losses_sum), min(train_losses_max), min(val_losses_max))\n",
        "global_max = max(max(train_losses_mean), max(val_losses_mean), max(train_losses_sum), max(val_losses_sum), max(train_losses_max), max(val_losses_max))\n",
        "\n",
        "\n",
        "# Plotting training and validation loss curves\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses_mean, label='Train Loss')\n",
        "plt.plot(val_losses_mean, label='Val Loss', linestyle='--')\n",
        "plt.title('Mean Aggregation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(global_min, global_max)  # Set uniform y-axis limits\n",
        "plt.legend()\n",
        "\n",
        "# Sum aggregation curves\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(train_losses_sum, label='Train Loss')\n",
        "plt.plot(val_losses_sum, label='Val Loss', linestyle='--')\n",
        "plt.title('Sum Aggregation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(global_min, global_max)  # Set uniform y-axis limits\n",
        "plt.legend()\n",
        "\n",
        "# Max aggregation curves\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(train_losses_max, label='Train Loss')\n",
        "plt.plot(val_losses_max, label='Val Loss', linestyle='--')\n",
        "plt.title('Max Aggregation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(global_min, global_max)  # Set uniform y-axis limits\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJlTa12KXoHH"
      },
      "outputs": [],
      "source": [
        "# Comparing the perfornace of the models\n",
        "\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Function to perform analysis\n",
        "def analyze_category_predictions(true_labels, pred_labels):\n",
        "    analysis_results = {}\n",
        "    for category in set(true_labels):\n",
        "        indices = [i for i, label in enumerate(true_labels) if label == category]\n",
        "        if not indices:\n",
        "            continue\n",
        "        category_preds = [pred_labels[i] for i in indices]\n",
        "        correct_preds_count = category_preds.count(category)\n",
        "        mispredictions = [pred for pred in category_preds if pred != category]\n",
        "        most_common_misprediction = Counter(mispredictions).most_common(1)\n",
        "        most_common_misprediction = most_common_misprediction[0] if most_common_misprediction else (\"None\", 0)\n",
        "\n",
        "        analysis_results[category] = {\n",
        "            'Correct Prediction Percentage': correct_preds_count / len(indices) * 100,\n",
        "            'Correct Prediction Count': correct_preds_count,\n",
        "            'Most Common Misprediction': most_common_misprediction[0],\n",
        "            'Misprediction Percentage': most_common_misprediction[1] / len(indices) * 100,\n",
        "            'Total Count': len(indices)\n",
        "        }\n",
        "    return analysis_results\n",
        "\n",
        "# Convert to human-readable labels\n",
        "all_true_labels = [index_product_dict[label.item()] for label in subset_graph.y.squeeze()]\n",
        "\n",
        "# Get predictions for all models\n",
        "def get_model_predictions(model, graph):\n",
        "    model.eval()\n",
        "    device = graph.x.device  # Ensure that model and data are on the same device\n",
        "    model.to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(graph.x.to(device), graph.edge_index.to(device))\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        return [index_product_dict[pred.item()] for pred in preds]\n",
        "\n",
        "# Analyze predictions for all models\n",
        "mean_preds = get_model_predictions(model_mean, subset_graph)\n",
        "sum_preds = get_model_predictions(model_sum, subset_graph)\n",
        "max_preds = get_model_predictions(model_max, subset_graph)\n",
        "\n",
        "mean_analysis = analyze_category_predictions(all_true_labels, mean_preds)\n",
        "sum_analysis = analyze_category_predictions(all_true_labels, sum_preds)\n",
        "max_analysis = analyze_category_predictions(all_true_labels, max_preds)\n",
        "\n",
        "# Create DataFrames from the analysis\n",
        "def create_dataframe(analysis_results):\n",
        "    data = [{\n",
        "        'Category': category,\n",
        "        'Correct Predictions (%)': f\"{stats['Correct Prediction Percentage']:.2f}% ({stats['Correct Prediction Count']})\",\n",
        "        'Most Common Misprediction': f\"{stats['Most Common Misprediction']} ({stats['Misprediction Percentage']:.2f}%)\",\n",
        "        'Total Count': stats['Total Count']\n",
        "    } for category, stats in analysis_results.items()]\n",
        "    return pd.DataFrame(data).sort_values('Total Count', ascending=False)\n",
        "\n",
        "mean_df = create_dataframe(mean_analysis)\n",
        "sum_df = create_dataframe(sum_analysis)\n",
        "max_df = create_dataframe(max_analysis)\n",
        "\n",
        "# Display the DataFrames\n",
        "print(\"Mean Aggregation Analysis\")\n",
        "print(mean_df)\n",
        "print(\"\\nSum Aggregation Analysis\")\n",
        "print(sum_df)\n",
        "print(\"\\nMax Aggregation Analysis\")\n",
        "print(max_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeleeFXQXoBY"
      },
      "outputs": [],
      "source": [
        "# Ensure 'Category' is a column and not an index\n",
        "mean_df = mean_df.reset_index(drop=True)\n",
        "sum_df = sum_df.reset_index(drop=True)\n",
        "max_df = max_df.reset_index(drop=True)\n",
        "\n",
        "# Merge the dataframes on 'Category' column\n",
        "comparison_df = pd.merge(mean_df, sum_df, on='Category', suffixes=('_Mean', '_Sum'))\n",
        "comparison_df = pd.merge(comparison_df, max_df, on='Category')\n",
        "comparison_df.rename(columns={\n",
        "    'Correct Predictions (%)': 'Correct Predictions (%)_Max',\n",
        "    'Most Common Misprediction': 'Most Common Misprediction_Max',\n",
        "    'Total Count': 'Total Count_Max'\n",
        "}, inplace=True)\n",
        "\n",
        "# Order the columns to group by Mean, Sum, Max\n",
        "column_order = ['Category',\n",
        "                'Correct Predictions (%)_Mean', 'Most Common Misprediction_Mean',\n",
        "                'Correct Predictions (%)_Sum', 'Most Common Misprediction_Sum',\n",
        "                'Correct Predictions (%)_Max', 'Most Common Misprediction_Max',\n",
        "                'Total Count_Mean']  # You can include the Total Counts for Sum and Max if they differ\n",
        "\n",
        "# Reindex the DataFrame based on the new column order\n",
        "comparison_df = comparison_df[column_order]\n",
        "\n",
        "# Display the combined DataFrame\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4flvY7tZfCHf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcOevEtEaB2y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import f1_score, log_loss\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, data, mask):\n",
        "    device = data.x.device  # Ensure that model and data are on the same device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize the loss function for negative log likelihood\n",
        "    loss_function = torch.nn.NLLLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(data.x.to(device), data.edge_index.to(device))\n",
        "\n",
        "        # Convert logits to log probabilities (required by NLLLoss)\n",
        "        log_probs = F.log_softmax(logits[mask], dim=1)\n",
        "\n",
        "        # Get the predicted classes for accuracy/F1 calculations\n",
        "        preds = logits[mask].argmax(dim=1)\n",
        "\n",
        "        true_labels = data.y[mask].to(device)\n",
        "\n",
        "        # Calculate F1 score using true labels and predictions\n",
        "        f1 = f1_score(true_labels.cpu().numpy(), preds.cpu().numpy(), average='weighted')\n",
        "\n",
        "        # Calculate negative log likelihood loss\n",
        "        nll_loss = loss_function(log_probs, true_labels)\n",
        "\n",
        "        # # Calculate log loss manually for comparison and potential use (this is informational)\n",
        "        # probabilities = torch.softmax(logits[mask], dim=1).cpu().numpy()\n",
        "        # logloss_sklearn = log_loss(true_labels.cpu().numpy(), probabilities, labels=np.unique(true_labels.cpu().numpy()))\n",
        "\n",
        "    return f1, nll_loss.item()\n",
        "\n",
        "\n",
        "# Define all possible labels explicitly (for example, labels from 0 to 31)\n",
        "all_labels = list(range(32))  # Adjust this range based on your specific dataset\n",
        "\n",
        "f1_score_mean, logloss_mean = evaluate(model_mean, subset_graph, subset_graph.val_mask)\n",
        "print(f\"Mean F1 Score: {f1_score_mean:.4f}, Log Loss: {logloss_mean:.4f}\" )\n",
        "\n",
        "f1_score_sum, logloss_sum = evaluate(model_sum, subset_graph, subset_graph.val_mask)\n",
        "print(f\"Sum F1 Score: {f1_score_sum:.4f}, Log Loss: {logloss_sum:.4f}\" )\n",
        "\n",
        "f1_score_max, logloss_max = evaluate(model_max, subset_graph, subset_graph.val_mask)\n",
        "print(f\"Max F1 Score: {f1_score_max:.4f}, Log Loss: {logloss_max:.4f}\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 5: Experimenting with Additional Aggregation Methods and Parameters\n",
        "In this section, we extend our experimentation with aggregation methods by testing layer-wise aggregations and advanced configurations. By using different combinations of aggregation strategies within the layers of the GraphSAGE model, we aim to identify which configurations yield the best performance.\n",
        "\n",
        "#### Layer-Wise Aggregation\n",
        "We experiment with applying different aggregation functions at each layer of the GraphSAGE model. This approach allows us to understand the impact of combining multiple aggregation methods within a single model architecture.\n",
        "\n",
        "#### Custom Aggregation Configurations\n",
        "We test several custom configurations:\n",
        "\n",
        "Sum-Max Aggregation: Applying sum aggregation in the first layer and max aggregation in the second layer.\n",
        "Mean-Max Aggregation: Applying mean aggregation in the first layer and max aggregation in the second layer.\n",
        "Multiple Aggregations: Using a combination of 'max', 'sum', and 'mean' aggregations within the layers.\n",
        "Advanced Aggregations: Incorporating advanced aggregation techniques like Softmax and Std aggregation.\n",
        "Jumping Knowledge Aggregation\n",
        "We also implement Jumping Knowledge (JK) networks, which aggregate information from multiple layers to improve the model's representational power.\n",
        "\n",
        "#### Training and Evaluation\n",
        "For each aggregation method and configuration, we train the model and evaluate its performance on a validation set. The training involves optimizing the model parameters using the Adam optimizer and computing the loss with CrossEntropyLoss.\n",
        "\n",
        "#### Performance Comparison\n",
        "We plot the training and validation loss curves for each aggregation method to compare their performance visually. Additionally, we calculate and compare the F1 scores and log loss values to quantitatively assess the effectiveness of each method."
      ],
      "metadata": {
        "id": "dO4jxD1j8g5g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k8DJWD5lsRj"
      },
      "outputs": [],
      "source": [
        "# layer-wise aggregation\n",
        "\n",
        "import torch\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class CustomGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(CustomGraphSAGE, self).__init__()\n",
        "        # First layer with mean aggregation\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='sum')\n",
        "        # Second layer with max aggregation\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='max')\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Apply the first layer and ReLU activation function\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        # Apply the second layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return torch.log_softmax(x, dim=1)\n",
        "\n",
        "# Instantiate the model and move it to the appropriate device\n",
        "model_layer_max_sum = CustomGraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes).to(device)\n",
        "\n",
        "# Create an optimizer for the model's parameters\n",
        "optimizer_layer = torch.optim.Adam(model_layer_max_sum.parameters(), lr=0.01)\n",
        "\n",
        "# Define the loss criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate the models\n",
        "train_losses_layer_max_sum, val_losses_layer_max_sum = train_and_evaluate(model_layer_max_sum, subset_graph, optimizer_layer, criterion, 200)\n",
        "\n",
        "# Plotting training and validation loss curves\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Mean aggregation curves\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses_layer_max_sum, label='Train Loss')\n",
        "plt.plot(val_losses_layer_max_sum, label='Val Loss', linestyle='--')\n",
        "plt.title('Layer_max_sum Aggregation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bSsmZkkn667"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPwDryPCbUyy"
      },
      "outputs": [],
      "source": [
        "# layer-wise aggregation\n",
        "\n",
        "import torch\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class CustomGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(CustomGraphSAGE, self).__init__()\n",
        "        # First layer with mean aggregation\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')\n",
        "        # Second layer with max aggregation\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='max')\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Apply the first layer and ReLU activation function\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        # Apply the second layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return torch.log_softmax(x, dim=1)\n",
        "\n",
        "# Instantiate the model and move it to the appropriate device\n",
        "model_layer_sum_mean = CustomGraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes).to(device)\n",
        "\n",
        "# Create an optimizer for the model's parameters\n",
        "optimizer_layer = torch.optim.Adam(model_layer_sum_mean.parameters(), lr=0.01)\n",
        "\n",
        "# Define the loss criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate the models\n",
        "train_losses_layer_sum_mean, val_losses_layer_sum_mean = train_and_evaluate(model_layer_sum_mean, subset_graph, optimizer_layer, criterion, 200)\n",
        "\n",
        "# Plotting training and validation loss curves\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Mean aggregation curves\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses_layer_sum_mean, label='Train Loss')\n",
        "plt.plot(val_losses_layer_sum_mean, label='Val Loss', linestyle='--')\n",
        "plt.title('Layer_sum_mean Aggregation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLYizpeYRy26"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rq-R9DOR0FP"
      },
      "outputs": [],
      "source": [
        "# layer-wise aggregation\n",
        "\n",
        "import torch\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class CustomGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(CustomGraphSAGE, self).__init__()\n",
        "        # First layer with mean aggregation\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')\n",
        "        # Second layer with max aggregation\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='max')\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Apply the first layer and ReLU activation function\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        # Apply the second layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return torch.log_softmax(x, dim=1)\n",
        "\n",
        "# Instantiate the model and move it to the appropriate device\n",
        "model_layer_sum_mean = CustomGraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes).to(device)\n",
        "\n",
        "# Create an optimizer for the model's parameters\n",
        "optimizer_layer = torch.optim.Adam(model_layer_sum_mean.parameters(), lr=0.01)\n",
        "\n",
        "# Define the loss criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate the models\n",
        "train_losses_layer_sum_mean, val_losses_layer_sum_mean = train_and_evaluate(model_layer_sum_mean, subset_graph, optimizer_layer, criterion, 200)\n",
        "\n",
        "# Plotting training and validation loss curves\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Mean aggregation curves\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses_layer_sum_mean, label='Train Loss')\n",
        "plt.plot(val_losses_layer_sum_mean, label='Val Loss', linestyle='--')\n",
        "plt.title('Layer_sum_mean Aggregation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgM1OqqORzfd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JP4QuLPR1W2"
      },
      "outputs": [],
      "source": [
        "# layer-wise aggregation\n",
        "\n",
        "import torch\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class CustomGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(CustomGraphSAGE, self).__init__()\n",
        "        # First layer with mean aggregation\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr=['max', 'sum', 'mean'])\n",
        "        # Second layer with max aggregation\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='mean')\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Apply the first layer and ReLU activation function\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        # Apply the second layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return torch.log_softmax(x, dim=1)\n",
        "\n",
        "# Instantiate the model and move it to the appropriate device\n",
        "model_layer_list1 = CustomGraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes).to(device)\n",
        "\n",
        "# Create an optimizer for the model's parameters\n",
        "optimizer_layer = torch.optim.Adam(model_layer_list1.parameters(), lr=0.01)\n",
        "\n",
        "# Define the loss criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate the models\n",
        "train_losses_layer_list1, val_losses_layer_list1 = train_and_evaluate(model_layer_list1, subset_graph, optimizer_layer, criterion, 200)\n",
        "\n",
        "# Plotting training and validation loss curves\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Mean aggregation curves\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses_layer_list1, label='Train Loss')\n",
        "plt.plot(val_losses_layer_list1, label='Val Loss', linestyle='--')\n",
        "plt.title('Layer_list1 Aggregation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXCufoVyR2jg"
      },
      "outputs": [],
      "source": [
        "# layer-wise aggregation\n",
        "\n",
        "import torch\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class CustomGraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(CustomGraphSAGE, self).__init__()\n",
        "        # First layer with mean aggregation\n",
        "        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr=[SoftmaxAggregation(), StdAggregation() ])\n",
        "        # Second layer with max aggregation\n",
        "        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='mean' )\n",
        "\n",
        "\n",
        "# model_10_layers = GCNWithCustomDropout(subset_graph.num_features, 64, dataset.num_classes, 2, dropout_rate=0.5, aggr=['max', 'sum', 'mean']).to(device)\n",
        "# model_50_layers = GCNWithCustomDropout(subset_graph.num_features, 64, dataset.num_classes, 2, dropout_rate=0.50, aggr=[SoftmaxAggregation(), StdAggregation() ] ).to(device)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # Apply the first layer and ReLU activation function\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "        # Apply the second layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return torch.log_softmax(x, dim=1)\n",
        "\n",
        "# Instantiate the model and move it to the appropriate device\n",
        "model_layer_list2 = CustomGraphSAGE(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes).to(device)\n",
        "\n",
        "# Create an optimizer for the model's parameters\n",
        "optimizer_layer = torch.optim.Adam(model_layer_list2.parameters(), lr=0.01)\n",
        "\n",
        "# Define the loss criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate the models\n",
        "train_losses_layer_list2, val_losses_layer_list2 = train_and_evaluate(model_layer_list2, subset_graph, optimizer_layer, criterion, 200)\n",
        "\n",
        "# Plotting training and validation loss curves\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Mean aggregation curves\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses_layer_list2, label='Train Loss')\n",
        "plt.plot(val_losses_layer_list2, label='Val Loss', linestyle='--')\n",
        "plt.title('Layer_sum_mean Aggregation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnT39UJoXI6V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNRMVtn_XI__"
      },
      "outputs": [],
      "source": [
        "# knowledge jumping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oZy4WMqXJCh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, JumpingKnowledge\n",
        "\n",
        "class CustomGCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super(CustomGCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
        "\n",
        "        # Initialize Jumping Knowledge with concatenation mode\n",
        "        self.jk = JumpingKnowledge(mode='cat')\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # List to save outputs from each layer for JK\n",
        "        layer_outputs = []\n",
        "\n",
        "        # First layer\n",
        "        x1 = self.conv1(x, edge_index)\n",
        "        x1 = F.relu(x1)\n",
        "        layer_outputs.append(x1)\n",
        "\n",
        "        # Second layer\n",
        "        x2 = self.conv2(x1, edge_index)\n",
        "        layer_outputs.append(x2)\n",
        "\n",
        "        # Apply Jumping Knowledge to aggregate layer outputs\n",
        "        x = self.jk(layer_outputs)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# Instantiate the model and move it to the appropriate device\n",
        "model_layer_jk = CustomGCN(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes).to(device)\n",
        "\n",
        "# Create an optimizer for the model's parameters\n",
        "optimizer_layer = torch.optim.Adam(model_layer_jk.parameters(), lr=0.01)  # Corrected to use model_layer_jk\n",
        "\n",
        "# Define the loss criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Function to train and evaluate the models\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, data, device, optimizer, criterion, epochs):\n",
        "    model.to(device)  # Ensure the model is on the correct device\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Ensure data is on the correct device\n",
        "        data.x = data.x.to(device)\n",
        "        data.edge_index = data.edge_index.to(device)\n",
        "        data.y = data.y.to(device)  # Move labels to GPU if using GPU\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data.x, data.edge_index)\n",
        "\n",
        "        # Only compute loss on the training mask\n",
        "        loss = criterion(output[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_output = model(data.x, data.edge_index)\n",
        "            val_loss = criterion(val_output[data.val_mask], data.y[data.val_mask])\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# # Dummy data for demonstration\n",
        "# train_losses_layer_jk = [0.5 - 0.005*i for i in range(200)]  # Example loss data\n",
        "# val_losses_layer_jk = [0.6 - 0.004*i for i in range(200)]  # Example validation loss data\n",
        "\n",
        "\n",
        "train_losses_layer_jk, val_losses_layer_jk = train_and_evaluate(model_layer_jk, subset_graph, device, optimizer_layer, criterion, 500)\n",
        "\n",
        "\n",
        "# Plotting training and validation loss curves\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot for Layer Jumping Knowledge aggregation\n",
        "plt.plot(train_losses_layer_jk, label='Train Loss')\n",
        "plt.plot(val_losses_layer_jk, label='Val Loss', linestyle='--')\n",
        "plt.title('Layer with Jumping Knowledge Aggregation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19bsAvBlz2TI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import f1_score, log_loss\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, data, mask):\n",
        "    device = data.x.device  # Ensure that model and data are on the same device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize the loss function for negative log likelihood\n",
        "    loss_function = torch.nn.NLLLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(data.x.to(device), data.edge_index.to(device))\n",
        "\n",
        "        # Convert logits to log probabilities (required by NLLLoss)\n",
        "        log_probs = F.log_softmax(logits[mask], dim=1)\n",
        "\n",
        "        # Get the predicted classes for accuracy/F1 calculations\n",
        "        preds = logits[mask].argmax(dim=1)\n",
        "\n",
        "        true_labels = data.y[mask].to(device)\n",
        "\n",
        "        # Calculate F1 score using true labels and predictions\n",
        "        f1 = f1_score(true_labels.cpu().numpy(), preds.cpu().numpy(), average='weighted')\n",
        "\n",
        "        # Calculate negative log likelihood loss\n",
        "        nll_loss = loss_function(log_probs, true_labels)\n",
        "\n",
        "        # # Calculate log loss manually for comparison and potential use (this is informational)\n",
        "        # probabilities = torch.softmax(logits[mask], dim=1).cpu().numpy()\n",
        "        # logloss_sklearn = log_loss(true_labels.cpu().numpy(), probabilities, labels=np.unique(true_labels.cpu().numpy()))\n",
        "\n",
        "    return f1, nll_loss.item()\n",
        "# Mean F1 Score: 0.7406, Log Loss: 2.1215\n",
        "\n",
        "# Define all possible labels explicitly (for example, labels from 0 to 31)\n",
        "all_labels = list(range(32))  # Adjust this range based on your specific dataset\n",
        "\n",
        "f1_score_mean, logloss_mean = evaluate(model_mean, subset_graph, subset_graph.val_mask)\n",
        "print(f\"Mean F1 Score: {f1_score_mean:.4f}, Log Loss: {logloss_mean:.4f}\" )\n",
        "\n",
        "f1_score_sum, logloss_sum = evaluate(model_sum, subset_graph, subset_graph.val_mask)\n",
        "print(f\"Sum F1 Score: {f1_score_sum:.4f}, Log Loss: {logloss_sum:.4f}\" )\n",
        "\n",
        "f1_score_max, logloss_max = evaluate(model_max, subset_graph, subset_graph.val_mask)\n",
        "print(f\"Max F1 Score: {f1_score_max:.4f}, Log Loss: {logloss_max:.4f}\" )\n",
        "\n",
        "f1_score_layer_sum_mean, logloss_layer_sum_mean = evaluate(model_layer_sum_mean, subset_graph, subset_graph.val_mask)\n",
        "print(f\"Layer_sum_mean F1 Score: {f1_score_layer_sum_mean:.4f}, Log Loss: {logloss_layer_sum_mean:.4f}\" )\n",
        "\n",
        "f1_score_layer_max_sum, logloss_layer_max_sum = evaluate(model_layer_max_sum, subset_graph, subset_graph.val_mask)\n",
        "print(f\"Layer_max_sum F1 Score: {f1_score_layer_max_sum:.4f}, Log Loss: {logloss_layer_max_sum:.4f}\" )\n",
        "\n",
        "f1_score_layer_list1, logloss_layer_list1 = evaluate(model_layer_list1, subset_graph, subset_graph.val_mask)\n",
        "print(f\"Layer_list1 F1 Score: {f1_score_layer_list1:.4f}, Log Loss: {logloss_layer_list1:.4f}\" )\n",
        "\n",
        "f1_score_layer_list2, logloss_layer_list2 = evaluate(model_layer_list2, subset_graph, subset_graph.val_mask)\n",
        "print(f\"Layer__list2 F1 Score: {f1_score_layer_list2:.4f}, Log Loss: {logloss_layer_list2:.4f}\" )\n",
        "\n",
        "f1_score_layer_jk, logloss_layer_jk = evaluate(model_layer_jk, subset_graph, subset_graph.val_mask)\n",
        "print(f\"Layer_jk F1 Score: {f1_score_layer_jk:.4f}, Log Loss: {logloss_layer_jk:.4f}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwjpN0mRz2V3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 6: Experimenting with Dropout\n",
        "In this section, we experiment with different dropout rates in the GCN model to understand their impact on model performance. Dropout is a regularization technique that helps prevent overfitting by randomly dropping out neurons during the training process. We aim to explore how various dropout rates affect the training and validation performance of our models.\n",
        "\n",
        "#### Dropout in GCN\n",
        "We modify the GCN model to include a dropout layer. This model will apply dropout at each layer, except the final one, to observe its effect on the model's ability to generalize.\n",
        "\n",
        "#### Training and Evaluation with and without Dropout\n",
        "We train and evaluate the model with a specific dropout rate and compare its performance against a model without dropout. This comparison helps us understand the benefits and potential drawbacks of using dropout in GCNs.\n",
        "\n",
        "#### Performance Comparison with Different Dropout Rates\n",
        "We experiment with different dropout rates (e.g., 0%, 50%, and 85%) to observe how each setting impacts the training and validation loss curves. By plotting these curves, we can visualize the effects of dropout on model convergence and stability.\n",
        "\n",
        "#### Summarizing Results\n",
        "Dropout Rates: We experiment with three different dropout rates—0% (no dropout), 50%, and 85%—to evaluate their impact on model performance.\n",
        "Training and Validation Losses: We plot the training and validation loss curves for each dropout rate to visualize how dropout affects model training and generalization.\n",
        "Performance Metrics: We calculate precision, recall, and F1 scores for each model to quantitatively assess the effectiveness of dropout in improving model performance.\n",
        "Confusion Matrix: We generate confusion matrices for each dropout rate to analyze the model's classification accuracy and error distribution."
      ],
      "metadata": {
        "id": "KOCoiND1chk0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osC1Gge80Fi4"
      },
      "outputs": [],
      "source": [
        "#dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeZuH4JM0G9N"
      },
      "outputs": [],
      "source": [
        "class GCNWithDropout(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout_rate=0.5):\n",
        "        super(GCNWithDropout, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList([GCNConv(in_channels, hidden_channels)])\n",
        "        self.dropout_rate = dropout_rate\n",
        "        for _ in range(1, num_layers-1):\n",
        "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.layers.append(GCNConv(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x, edge_index))\n",
        "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "        x = self.layers[-1](x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZeQNQiR0Hx5"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model and move it to the appropriate device\n",
        "model_layer = GCNWithDropout(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes).to(device)\n",
        "\n",
        "# Create an optimizer for the model's parameters\n",
        "optimizer_layer = torch.optim.Adam(model_layer.parameters(), lr=0.01)\n",
        "\n",
        "# Define the loss criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train_and_evaluate(model, data, optimizer, criterion, epochs):\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_model(model, data, optimizer, criterion)\n",
        "        val_loss = evaluate_model(model, data, criterion)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Train and evaluate the models\n",
        "train_losses_layer, val_losses_layer = train_and_evaluate(model_layer, subset_graph, optimizer_layer, criterion, 200)\n",
        "\n",
        "# Plotting training and validation loss curves\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Mean aggregation curves\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(train_losses_layer, label='Train Loss')\n",
        "plt.plot(val_losses_layer, label='Val Loss', linestyle='--')\n",
        "plt.title('Mean Aggregation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH__cbyJ1gZ1"
      },
      "outputs": [],
      "source": [
        "#compare dropout with no-dropout\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCNWithOptionalDropout(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout_rate=0):\n",
        "        super(GCNWithOptionalDropout, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList([GCNConv(in_channels, hidden_channels)])\n",
        "        self.dropout_rate = dropout_rate\n",
        "        for _ in range(1, num_layers-1):\n",
        "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.layers.append(GCNConv(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x, edge_index))\n",
        "            if self.dropout_rate > 0:  # Apply dropout only if rate is greater than 0\n",
        "                x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "        x = self.layers[-1](x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Model without dropout\n",
        "model_no_dropout = GCNWithOptionalDropout(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, num_layers=3, dropout_rate=0).to(device)\n",
        "\n",
        "# Model with dropout\n",
        "model_with_dropout = GCNWithOptionalDropout(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, num_layers=3, dropout_rate=.8).to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_no_dropout = torch.optim.Adam(model_no_dropout.parameters(), lr=0.01)\n",
        "optimizer_with_dropout = torch.optim.Adam(model_with_dropout.parameters(), lr=0.01)\n",
        "\n",
        "# Training and evaluating both models\n",
        "train_losses_no_dropout, val_losses_no_dropout = train_and_evaluate(model_no_dropout, subset_graph, optimizer_no_dropout, criterion, 200)\n",
        "train_losses_with_dropout, val_losses_with_dropout = train_and_evaluate(model_with_dropout, subset_graph, optimizer_with_dropout, criterion, 200)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Training and validation losses without dropout\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses_no_dropout, label='Train Loss - No Dropout')\n",
        "plt.plot(val_losses_no_dropout, label='Val Loss - No Dropout', linestyle='--')\n",
        "plt.title('Performance without Dropout')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Training and validation losses with dropout\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_losses_with_dropout, label='Train Loss - With Dropout')\n",
        "plt.plot(val_losses_with_dropout, label='Val Loss - With Dropout', linestyle='--')\n",
        "plt.title('Performance with Dropout')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlHZqiEF2PdC"
      },
      "outputs": [],
      "source": [
        "# 3 dropout cases\n",
        "\n",
        "class GCNWithCustomDropout(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=3, dropout_rate=0):\n",
        "        super(GCNWithCustomDropout, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList([GCNConv(in_channels, hidden_channels)])\n",
        "        self.dropout_rate = dropout_rate\n",
        "        for _ in range(1, num_layers-1):\n",
        "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.layers.append(GCNConv(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x, edge_index))\n",
        "            if self.dropout_rate > 0:  # Apply dropout only if rate is greater than 0\n",
        "                x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "        x = self.layers[-1](x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Models with different dropout rates\n",
        "model_dropout_0 = GCNWithCustomDropout(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, dropout_rate=0).to(device)\n",
        "model_dropout_05 = GCNWithCustomDropout(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, dropout_rate=0.5).to(device)\n",
        "model_dropout_085 = GCNWithCustomDropout(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, dropout_rate=0.85).to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_0 = torch.optim.Adam(model_dropout_0.parameters(), lr=0.01)\n",
        "optimizer_05 = torch.optim.Adam(model_dropout_05.parameters(), lr=0.01)\n",
        "optimizer_085 = torch.optim.Adam(model_dropout_085.parameters(), lr=0.01)\n",
        "\n",
        "# Training and evaluating all models\n",
        "train_losses_0, val_losses_0 = train_and_evaluate(model_dropout_0, subset_graph, optimizer_0, criterion, 200)\n",
        "train_losses_05, val_losses_05 = train_and_evaluate(model_dropout_05, subset_graph, optimizer_05, criterion, 200)\n",
        "train_losses_085, val_losses_085 = train_and_evaluate(model_dropout_085, subset_graph, optimizer_085, criterion, 200)\n",
        "\n",
        "max_epoch = 200\n",
        "\n",
        "# Assuming you have data for your plots:\n",
        "# train_losses_0, val_losses_0, train_losses_05, val_losses_05, train_losses_085, val_losses_085\n",
        "\n",
        "# Determine common limits for x and y axes\n",
        "all_losses = train_losses_0 + val_losses_0 + train_losses_05 + val_losses_05 + train_losses_085 + val_losses_085\n",
        "x_limits = (0, max_epoch)  # replace max_epoch with your actual max epoch\n",
        "y_limits = (min(all_losses), max(all_losses))\n",
        "\n",
        "# Create subplots\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Plot for 0% dropout\n",
        "ax1.plot(train_losses_0, label='Train Loss - 0% Dropout')\n",
        "ax1.plot(val_losses_0, label='Val Loss - 0% Dropout', linestyle='--')\n",
        "ax1.set_title('Performance with 0% Dropout')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.set_xlim(x_limits)\n",
        "ax1.set_ylim(y_limits)\n",
        "\n",
        "# Plot for 50% dropout\n",
        "ax2.plot(train_losses_05, label='Train Loss - 50% Dropout')\n",
        "ax2.plot(val_losses_05, label='Val Loss - 50% Dropout', linestyle='--')\n",
        "ax2.set_title('Performance with 50% Dropout')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.set_xlim(x_limits)\n",
        "ax2.set_ylim(y_limits)\n",
        "\n",
        "# Plot for 85% dropout\n",
        "ax3.plot(train_losses_085, label='Train Loss - 85% Dropout')\n",
        "ax3.plot(val_losses_085, label='Val Loss - 85% Dropout', linestyle='--')\n",
        "ax3.set_title('Performance with 85% Dropout')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Loss')\n",
        "ax3.legend()\n",
        "ax3.set_xlim(x_limits)\n",
        "ax3.set_ylim(y_limits)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayaGxXAt22QH"
      },
      "outputs": [],
      "source": [
        "# Function to get predictions from a model\n",
        "def get_predictions(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x.to(device), data.edge_index.to(device))\n",
        "        preds = torch.argmax(out, dim=1)\n",
        "        return [index_product_dict[pred.item()] for pred in preds]\n",
        "\n",
        "all_true_labels = [index_product_dict[label.item()] for label in subset_graph.y.squeeze()]\n",
        "\n",
        "\n",
        "# Analyze predictions from models with different dropout rates\n",
        "predictions_0_dropout = get_predictions(model_dropout_0, subset_graph)\n",
        "predictions_50_dropout = get_predictions(model_dropout_05, subset_graph)\n",
        "predictions_85_dropout = get_predictions(model_dropout_085, subset_graph)\n",
        "\n",
        "# Perform analysis for each dropout rate\n",
        "analysis_0_dropout = analyze_category_predictions(all_true_labels, predictions_0_dropout)\n",
        "analysis_50_dropout = analyze_category_predictions(all_true_labels, predictions_50_dropout)\n",
        "analysis_85_dropout = analyze_category_predictions(all_true_labels, predictions_85_dropout)\n",
        "\n",
        "# Convert the analysis results into DataFrames\n",
        "df_0_dropout = create_dataframe(analysis_0_dropout)\n",
        "df_50_dropout = create_dataframe(analysis_50_dropout)\n",
        "df_85_dropout = create_dataframe(analysis_85_dropout)\n",
        "\n",
        "# Merge the DataFrames for comparison\n",
        "comparison_df = pd.merge(df_0_dropout, df_50_dropout, on='Category', suffixes=('_0', '_50'))\n",
        "comparison_df = pd.merge(comparison_df, df_85_dropout, on='Category')\n",
        "comparison_df.rename(columns={\n",
        "    'Correct Predictions (%)': 'Correct Predictions (%)_85',\n",
        "    'Most Common Misprediction': 'Most Common Misprediction_85',\n",
        "    'Total Count': 'Total Count_85'\n",
        "}, inplace=True)\n",
        "\n",
        "# Set the desired order for the columns\n",
        "column_order = ['Category',\n",
        "                'Correct Predictions (%)_0', 'Most Common Misprediction_0', 'Total Count_0',\n",
        "                'Correct Predictions (%)_50', 'Most Common Misprediction_50', 'Total Count_50',\n",
        "                'Correct Predictions (%)_85', 'Most Common Misprediction_85', 'Total Count_85']\n",
        "\n",
        "# Reindex the DataFrame based on the new column order and display it\n",
        "comparison_df = comparison_df[column_order]\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcu7EuCuIlD4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vzPYPANIk_x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_model(model, data):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x.to(device), data.edge_index.to(device))  # Forward pass\n",
        "        preds = torch.argmax(out, dim=1)[data.test_mask]  # Only take the predictions from the test mask\n",
        "        true_labels = data.y[data.test_mask]  # True labels for the test data\n",
        "    return preds.cpu().numpy(), true_labels.cpu().numpy()\n",
        "\n",
        "# Assuming the model is already trained and test_data is properly configured\n",
        "predictions, true_labels = test_model(model_dropout_05, subset_graph)\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "precision = precision_score(true_labels, predictions, average='weighted')\n",
        "recall = recall_score(true_labels, predictions, average='weighted')\n",
        "f1 = f1_score(true_labels, predictions, average='weighted')\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "# Display the metrics\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R329NxTeIk8U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYr4OoqGIk43"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mdFTv5Z-mAy"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "#\n",
        "# Create the test dataset by filtering out the test nodes\n",
        "test_data = subset_graph.clone()  # Clone the graph data to preserve the original graph\n",
        "test_data.x = subset_graph.x[subset_graph.test_mask]  # Node features for test set\n",
        "test_data.edge_index = subset_graph.edge_index  # Edge index is typically the same for the test set\n",
        "test_data.y = subset_graph.y[subset_graph.test_mask]  # Labels for the test set\n",
        "\n",
        "# Make sure to adjust edge_index to only include edges from the test set nodes if necessary\n",
        "\n",
        "# Now you can use test_data to get predictions from your models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPE24hLELNnv"
      },
      "outputs": [],
      "source": [
        "# Assuming test_data is prepared with the correct test mask and device setup\n",
        "\n",
        "# Test and evaluate each model\n",
        "preds_0, labels_0 = test_model(model_dropout_0, subset_graph)\n",
        "preds_05, labels_05 = test_model(model_dropout_05, subset_graph)\n",
        "preds_085, labels_085 = test_model(model_dropout_085, subset_graph)\n",
        "\n",
        "# Compute metrics for each model\n",
        "precision_0 = precision_score(labels_0, preds_0, average='weighted')\n",
        "recall_0 = recall_score(labels_0, preds_0, average='weighted')\n",
        "f1_0 = f1_score(labels_0, preds_0, average='weighted')\n",
        "cm_0 = confusion_matrix(labels_0, preds_0)\n",
        "\n",
        "precision_05 = precision_score(labels_05, preds_05, average='weighted')\n",
        "recall_05 = recall_score(labels_05, preds_05, average='weighted')\n",
        "f1_05 = f1_score(labels_05, preds_05, average='weighted')\n",
        "cm_05 = confusion_matrix(labels_05, preds_05)\n",
        "\n",
        "precision_085 = precision_score(labels_085, preds_085, average='weighted')\n",
        "recall_085 = recall_score(labels_085, preds_085, average='weighted')\n",
        "f1_085 = f1_score(labels_085, preds_085, average='weighted')\n",
        "cm_085 = confusion_matrix(labels_085, preds_085)\n",
        "\n",
        "# Print metrics for 0% Dropout\n",
        "print(\"0% Dropout:\")\n",
        "print(f\"Precision: {precision_0:.4f}, Recall: {recall_0:.4f}, F1 Score: {f1_0:.4f}\")\n",
        "sns.heatmap(cm_0, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix - 0% Dropout')\n",
        "plt.show()\n",
        "\n",
        "# Print metrics for 50% Dropout\n",
        "print(\"50% Dropout:\")\n",
        "print(f\"Precision: {precision_05:.4f}, Recall: {recall_05:.4f}, F1 Score: {f1_05:.4f}\")\n",
        "sns.heatmap(cm_05, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix - 50% Dropout')\n",
        "plt.show()\n",
        "\n",
        "# Print metrics for 85% Dropout\n",
        "print(\"85% Dropout:\")\n",
        "print(f\"Precision: {precision_085:.4f}, Recall: {recall_085:.4f}, F1 Score: {f1_085:.4f}\")\n",
        "sns.heatmap(cm_085, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix - 85% Dropout')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5vgXulME2FZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmX_Yj0jODbH"
      },
      "outputs": [],
      "source": [
        "# learning rate, depth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 7: Experimenting with Model Depth\n",
        "In this section, we experiment with the depth of the GCN by varying the number of layers in the model. Model depth can significantly impact the learning capacity and generalization ability of neural networks. We aim to explore how different depths affect the training and validation performance of our models.\n",
        "\n",
        "#### Variable Depth in GCN\n",
        "We modify the GCN model to accept a variable number of layers. This flexibility allows us to create models with different depths and evaluate their performance on the same dataset.\n",
        "\n",
        "#### Training and Evaluation with Different Depths\n",
        "We initialize three models with varying depths:\n",
        "\n",
        "2 Layers: A shallow model with one hidden layer.\n",
        "10 Layers: A moderately deep model with multiple hidden layers.\n",
        "50 Layers: A very deep model with many hidden layers.\n",
        "Each model is trained and evaluated using the same training and validation procedures to ensure a fair comparison.\n",
        "\n",
        "#### Performance Comparison\n",
        "We compare the training and validation loss curves for each model depth to understand how increasing the number of layers affects the model's ability to learn and generalize. By plotting these curves, we can visualize the impact of model depth on convergence and stability.\n",
        "\n",
        "#### Summary of Results\n",
        "Model Depths: We experiment with three different depths—2 layers, 10 layers, and 50 layers—to evaluate their impact on model performance.\n",
        "Training and Validation Losses: We plot the training and validation loss curves for each model depth to visualize how the number of layers affects the learning process.\n",
        "Performance Metrics: We calculate and compare the training and validation losses to assess the effectiveness of each model depth."
      ],
      "metadata": {
        "id": "-9nTIEBWd0CA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXRrcUX7Pnfp"
      },
      "outputs": [],
      "source": [
        "class GCNWithVariableDepth(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2):\n",
        "        super(GCNWithVariableDepth, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        # Base layer\n",
        "        self.layers.append(GCNConv(in_channels, hidden_channels))\n",
        "        # Hidden layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        # Output layer\n",
        "        self.layers.append(GCNConv(hidden_channels, out_channels))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x, edge_index))\n",
        "        x = self.layers[-1](x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkwDXFqNPt58"
      },
      "outputs": [],
      "source": [
        "# Initialize models with different depths\n",
        "model_2_layers = GCNWithVariableDepth(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, num_layers=2).to(device)\n",
        "model_10_layers = GCNWithVariableDepth(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, num_layers=5).to(device)\n",
        "model_50_layers = GCNWithVariableDepth(in_channels=subset_graph.num_features, hidden_channels=64, out_channels=dataset.num_classes, num_layers=20).to(device)\n",
        "\n",
        "optimizer_2 = torch.optim.Adam(model_2_layers.parameters(), lr=0.01)\n",
        "optimizer_10 = torch.optim.Adam(model_10_layers.parameters(), lr=0.01)\n",
        "optimizer_50 = torch.optim.Adam(model_50_layers.parameters(), lr=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19gQCnwKPwXf"
      },
      "outputs": [],
      "source": [
        "# Example training and evaluation function\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_and_evaluate(model, data, device, optimizer, criterion, epochs):\n",
        "    model.to(device)  # Ensure the model is on the correct device\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Ensure data is on the correct device\n",
        "        data.x = data.x.to(device)\n",
        "        data.edge_index = data.edge_index.to(device)\n",
        "        data.y = data.y.to(device)  # Move labels to GPU if using GPU\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(data.x, data.edge_index)\n",
        "\n",
        "        # Only compute loss on the training mask\n",
        "        loss = criterion(output[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_output = model(data.x, data.edge_index)\n",
        "            val_loss = criterion(val_output[data.val_mask], data.y[data.val_mask])\n",
        "\n",
        "        train_losses.append(loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CId3RAsRPyOP"
      },
      "outputs": [],
      "source": [
        "# Assuming 'train_and_evaluate' is defined to include training and validation\n",
        "train_and_evaluate(model_2_layers, subset_graph, device, epochs=100, optimizer=optimizer_2, criterion=torch.nn.CrossEntropyLoss())\n",
        "train_and_evaluate(model_10_layers, subset_graph, device, epochs=100, optimizer=optimizer_10, criterion=torch.nn.CrossEntropyLoss())\n",
        "train_and_evaluate(model_50_layers, subset_graph, device, epochs=100, optimizer=optimizer_50, criterion=torch.nn.CrossEntropyLoss())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3FsIMAiP1P0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_and_evaluate(model, data, device, optimizer, criterion, epochs):\n",
        "    model.to(device)  # Ensure the model is on the correct device\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Move data to the correct device\n",
        "        data.x = data.x.to(device)\n",
        "        data.edge_index = data.edge_index.to(device)\n",
        "        data.y = data.y.to(device)  # Move labels to GPU if using GPU\n",
        "\n",
        "        # Forward pass and loss calculation on the training mask\n",
        "        output = model(data.x, data.edge_index)\n",
        "        train_loss = criterion(output[data.train_mask], data.y[data.train_mask])\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation loss calculation on the validation mask\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_output = model(data.x, data.edge_index)\n",
        "            val_loss = criterion(val_output[data.val_mask], data.y[data.val_mask])\n",
        "\n",
        "        train_losses.append(train_loss.item())\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}: Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Models with 2, 10, and 50 layers\n",
        "train_losses_2, val_losses_2 = train_and_evaluate(model_2_layers, subset_graph, device, optimizer_2, torch.nn.CrossEntropyLoss(), 100)\n",
        "train_losses_10, val_losses_10 = train_and_evaluate(model_10_layers, subset_graph, device, optimizer_10, torch.nn.CrossEntropyLoss(), 100)\n",
        "train_losses_50, val_losses_50 = train_and_evaluate(model_50_layers, subset_graph, device, optimizer_50, torch.nn.CrossEntropyLoss(), 100)\n",
        "\n",
        "# Printing training and validation losses for each model configuration\n",
        "print(\"Model with 2 layers - Train Losses:\", train_losses_2)\n",
        "print(\"Model with 2 layers - Validation Losses:\", val_losses_2)\n",
        "\n",
        "print(\"Model with 10 layers - Train Losses:\", train_losses_10)\n",
        "print(\"Model with 10 layers - Validation Losses:\", val_losses_10)\n",
        "\n",
        "print(\"Model with 50 layers - Train Losses:\", train_losses_50)\n",
        "print(\"Model with 50 layers - Validation Losses:\", val_losses_50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AECXULyiVhOW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(train_losses, val_losses, title):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss', linestyle='--')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Assuming you have already collected the losses in the variables below\n",
        "# train_losses_2, val_losses_2\n",
        "# train_losses_10, val_losses_10\n",
        "# train_losses_50, val_losses_50\n",
        "\n",
        "plot_losses(train_losses_2, val_losses_2, \"Training and Validation Losses for 2 Layers\")\n",
        "plot_losses(train_losses_10, val_losses_10, \"Training and Validation Losses for 10 Layers\")\n",
        "plot_losses(train_losses_50, val_losses_50, \"Training and Validation Losses for 50 Layers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wVmc33SifNdA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QJ74rq7YfNEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 8: Combining Concepts into a Comprehensive Model\n",
        "In this section, we integrate the various concepts explored earlier, including dropout, aggregation methods, and model depth, into a comprehensive model. This combined approach allows us to leverage the strengths of each technique to optimize our Graph Convolutional Network (GCN) performance.\n",
        "\n",
        "#### Combined Model Architecture\n",
        "We utilize a flexible GCN model that incorporates dropout, different aggregation strategies, and variable depth. This model aims to balance the trade-offs between model complexity, regularization, and representational power.\n",
        "\n",
        "#### Model Initialization\n",
        "We initialize three models with varying depths (2, 10, and 50 layers) and different dropout rates:\n",
        "\n",
        "2 Layers: Dropout rate of 50%, Softmax Aggregation.\n",
        "10 Layers: Dropout rate of 50%, combination of 'max', 'sum', and 'mean' aggregations.\n",
        "50 Layers: Dropout rate of 50%, advanced aggregation techniques (Softmax and Std Aggregation).\n",
        "Training and Evaluation\n",
        "Each model is trained and evaluated using the same dataset and training procedure. We record the training and validation losses and compute performance metrics such as precision, recall, F1 score, and log loss.\n",
        "\n",
        "#### Performance Comparison\n",
        "We compare the training and validation loss curves, as well as the performance metrics, for each model to understand the impact of combining dropout, aggregation methods, and varying depths on the overall model performance."
      ],
      "metadata": {
        "id": "bm5NjJLOfMrJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9ff2-dDVuxk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class GCNWithCustomDropout(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout_rate=0.5):\n",
        "        super(GCNWithCustomDropout, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList([GCNConv(in_channels, hidden_channels)])\n",
        "        for _ in range(1, num_layers-1):\n",
        "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.layers.append(GCNConv(hidden_channels, out_channels))\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x, edge_index))\n",
        "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "        x = self.layers[-1](x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "def train_and_evaluate(model, data, device, optimizer, criterion, epochs):\n",
        "    train_losses, val_losses = [], []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x.to(device), data.edge_index.to(device))\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask].to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data.x.to(device), data.edge_index.to(device))\n",
        "            val_loss = criterion(out[data.val_mask], data.y[data.val_mask].to(device))\n",
        "            val_losses.append(val_loss.item())\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Initialize models with 50% dropout\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_2_layers = GCNWithCustomDropout(subset_graph.num_features, 64, dataset.num_classes, 2, dropout_rate=0.5).to(device)\n",
        "model_10_layers = GCNWithCustomDropout(subset_graph.num_features, 64, dataset.num_classes, 10, dropout_rate=0.5).to(device)\n",
        "model_50_layers = GCNWithCustomDropout(subset_graph.num_features, 64, dataset.num_classes, 50, dropout_rate=0.5).to(device)\n",
        "\n",
        "# Create optimizers and criterion\n",
        "optimizer_2 = torch.optim.Adam(model_2_layers.parameters(), lr=0.01)\n",
        "optimizer_10 = torch.optim.Adam(model_10_layers.parameters(), lr=0.01)\n",
        "optimizer_50 = torch.optim.Adam(model_50_layers.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate each model\n",
        "train_losses_2, val_losses_2 = train_and_evaluate(model_2_layers, subset_graph, device, optimizer_2, criterion, 100)\n",
        "train_losses_10, val_losses_10 = train_and_evaluate(model_10_layers, subset_graph, device, optimizer_10, criterion, 100)\n",
        "train_losses_50, val_losses_50 = train_and_evaluate(model_50_layers, subset_graph, device, optimizer_50, criterion, 100)\n",
        "\n",
        "# Plot the losses\n",
        "plot_losses(train_losses_2, val_losses_2, \"Training and Validation Losses for 2 Layers with 50% Dropout\")\n",
        "plot_losses(train_losses_10, val_losses_10, \"Training and Validation Losses for 10 Layers with 50% Dropout\")\n",
        "plot_losses(train_losses_50, val_losses_50, \"Training and Validation Losses for 50 Layers with 50% Dropout\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JooXNuVXXHsR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class GCNWithCustomDropout(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout_rate=0.5):\n",
        "        super(GCNWithCustomDropout, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList([GCNConv(in_channels, hidden_channels)])\n",
        "        for _ in range(1, num_layers-1):\n",
        "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.layers.append(GCNConv(hidden_channels, out_channels))\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x, edge_index))\n",
        "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "        x = self.layers[-1](x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "def train_and_evaluate(model, data, device, optimizer, criterion, epochs):\n",
        "    train_losses, val_losses = [], []\n",
        "    best_metrics = {'precision': 0, 'recall': 0, 'f1_score': 0}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x.to(device), data.edge_index.to(device))\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask].to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data.x.to(device), data.edge_index.to(device))\n",
        "            val_loss = criterion(out[data.val_mask], data.y[data.val_mask].to(device))\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            preds = torch.argmax(out[data.test_mask], dim=1)\n",
        "            labels = data.y[data.test_mask].to(device)\n",
        "            precision, recall, f1_score, _ = precision_recall_fscore_support(labels.cpu(), preds.cpu(), average='macro')\n",
        "\n",
        "            # Update best metrics\n",
        "            if f1_score > best_metrics['f1_score']:\n",
        "                best_metrics.update({'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "\n",
        "    return train_losses, val_losses, best_metrics\n",
        "\n",
        "def evaluate_predictions(probs, labels):\n",
        "    epsilon = 1e-9  # Small constant to prevent log(0)\n",
        "    probs_clamped = torch.clamp(probs, epsilon, 1-epsilon)\n",
        "    log_probs = torch.log(probs_clamped)\n",
        "    loss_function = torch.nn.NLLLoss()\n",
        "    log_loss = loss_function(log_probs, labels)\n",
        "    return log_loss.item()\n",
        "\n",
        "def plot_losses(train_losses, val_losses, best_metrics, title):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss', linestyle='--')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.ylim(0, max(max(train_losses), max(val_losses)) + 0.05)  # Adjust y-limit\n",
        "    plt.legend()\n",
        "    # plt.text(0.5, 0.1, f'Best Precision: {best_metrics[\"precision\"]:.4f}\\n'\n",
        "    #                    f'Best Recall: {best_metrics[\"recall\"]:.4f}\\n'\n",
        "    #                    f'Best F1 Score: {best_metrics[\"f1_score\"]:.4f}', horizontalalignment='center',\n",
        "    #          verticalalignment='center', transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
        "    plt.show()\n",
        "\n",
        "# Initialize and train models as before, then plot with the new `plot_losses` function including best metrics annotations.\n",
        "\n",
        "# Initialize models with 50% dropout\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_2_layers = GCNWithCustomDropout(subset_graph.num_features, 64, dataset.num_classes, 2, dropout_rate=0).to(device)\n",
        "model_10_layers = GCNWithCustomDropout(subset_graph.num_features, 64, dataset.num_classes, 2, dropout_rate=0.25).to(device)\n",
        "model_50_layers = GCNWithCustomDropout(subset_graph.num_features, 64, dataset.num_classes, 2, dropout_rate=0.50).to(device)\n",
        "\n",
        "# Create optimizers and criterion\n",
        "optimizer_2 = torch.optim.Adam(model_2_layers.parameters(), lr=0.01)\n",
        "optimizer_10 = torch.optim.Adam(model_10_layers.parameters(), lr=0.01)\n",
        "optimizer_50 = torch.optim.Adam(model_50_layers.parameters(), lr=0.01)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate each model\n",
        "train_losses_2, val_losses_2, best_metrics_2 = train_and_evaluate(model_2_layers, subset_graph, device, optimizer_2, criterion, 12000)\n",
        "train_losses_10, val_losses_10 , best_metrics_10= train_and_evaluate(model_10_layers, subset_graph, device, optimizer_10, criterion, 12000)\n",
        "train_losses_50, val_losses_50 , best_metrics_50 = train_and_evaluate(model_50_layers, subset_graph, device, optimizer_50, criterion, 12000)\n",
        "\n",
        "# Plot the losses with aligned axes\n",
        "max_loss = max(max(train_losses_2 + val_losses_2), max(train_losses_10 + val_losses_10), max(train_losses_50 + val_losses_50))\n",
        "\n",
        "\n",
        "# Plot the losses\n",
        "plot_losses(train_losses_2, val_losses_2, best_metrics_2, \"Training and Validation Losses for 2 Layers with 50% Dropout\")\n",
        "plot_losses(train_losses_10, val_losses_10, best_metrics_10, \"Training and Validation Losses for 10 Layers with 50% Dropout\")\n",
        "plot_losses(train_losses_50, val_losses_50, best_metrics_50, \"Training and Validation Losses for 50 Layers with 50% Dropout\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17BA18SgYGWW"
      },
      "outputs": [],
      "source": [
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtYuUWJCcl9M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_model(model, data, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(data.x.to(device), data.edge_index.to(device))\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "        return preds, data.y.to(device), probs\n",
        "\n",
        "def evaluate_predictions(probs, labels):\n",
        "    epsilon = 1e-9  # Small constant to prevent log(0)\n",
        "    probs_clamped = torch.clamp(probs, epsilon, 1-epsilon)\n",
        "    log_probs = torch.log(probs_clamped)\n",
        "    loss_function = torch.nn.NLLLoss()\n",
        "    log_loss = loss_function(log_probs, labels)\n",
        "    return log_loss.item()\n",
        "\n",
        "\n",
        "# Testing and evaluation of models\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "preds_2, labels_2, probs_2 = test_model(model_2_layers, subset_graph, device)\n",
        "preds_10, labels_10, probs_10 = test_model(model_10_layers, subset_graph, device)\n",
        "preds_50, labels_50, probs_50 = test_model(model_50_layers, subset_graph, device)\n",
        "\n",
        "log_loss_2 = evaluate_predictions(probs_2, labels_2)\n",
        "log_loss_10 = evaluate_predictions(probs_10, labels_10)\n",
        "log_loss_50 = evaluate_predictions(probs_50, labels_50)\n",
        "\n",
        "# Compute metrics for each model\n",
        "precision_2 = precision_score(labels_2.cpu().numpy(), preds_2.cpu().numpy(), average='weighted')\n",
        "recall_2 = recall_score(labels_2.cpu().numpy(), preds_2.cpu().numpy(), average='weighted')\n",
        "f1_2 = f1_score(labels_2.cpu().numpy(), preds_2.cpu().numpy(), average='weighted')\n",
        "cm_2 = confusion_matrix(labels_2.cpu().numpy(), preds_2.cpu().numpy())\n",
        "\n",
        "precision_10 = precision_score(labels_10.cpu().numpy(), preds_10.cpu().numpy(), average='weighted')\n",
        "recall_10 = recall_score(labels_10.cpu().numpy(), preds_10.cpu().numpy(), average='weighted')\n",
        "f1_10 = f1_score(labels_10.cpu().numpy(), preds_10.cpu().numpy(), average='weighted')\n",
        "cm_10 = confusion_matrix(labels_10.cpu().numpy(), preds_10.cpu().numpy())\n",
        "\n",
        "precision_50 = precision_score(labels_50.cpu().numpy(), preds_50.cpu().numpy(), average='weighted')\n",
        "recall_50 = recall_score(labels_50.cpu().numpy(), preds_50.cpu().numpy(), average='weighted')\n",
        "f1_50 = f1_score(labels_50.cpu().numpy(), preds_50.cpu().numpy(), average='weighted')\n",
        "cm_50 = confusion_matrix(labels_50.cpu().numpy(), preds_50.cpu().numpy())\n",
        "\n",
        "# Print metrics for each model\n",
        "print(\"2-Layer:\")\n",
        "print(f\"Precision: {precision_2:.4f}, Recall: {recall_2:.4f}, F1 Score: {f1_2:.4f}, Log Loss: {log_loss_2:.4f}\")\n",
        "sns.heatmap(cm_2, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix - 0% Dropout')\n",
        "plt.show()\n",
        "\n",
        "print(\"10-Layer:\")\n",
        "print(f\"Precision: {precision_10:.4f}, Recall: {recall_10:.4f}, F1 Score: {f1_10:.4f}, Log Loss: {log_loss_10:.4f}\")\n",
        "sns.heatmap(cm_10, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix - 50% Dropout')\n",
        "plt.show()\n",
        "\n",
        "print(\"50-Layer:\")\n",
        "print(f\"Precision: {precision_50:.4f}, Recall: {recall_50:.4f}, F1 Score: {f1_50:.4f}, Log Loss: {log_loss_50:.4f}\")\n",
        "sns.heatmap(cm_50, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix - 85% Dropout')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXCLin430r1X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cglTN5qLDkY_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIrm5uvDDkbu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AasKtZIrDkoQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class GCNWithCustomDropout(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout_rate=0.5, aggr = 'mean'):\n",
        "        super(GCNWithCustomDropout, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList([GCNConv(in_channels, hidden_channels)])\n",
        "        for _ in range(1, num_layers-1):\n",
        "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.layers.append(GCNConv(hidden_channels, out_channels))\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = F.relu(layer(x, edge_index))\n",
        "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "        x = self.layers[-1](x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "def train_and_evaluate(model, data, device, optimizer, criterion, epochs):\n",
        "    train_losses, val_losses = [], []\n",
        "    best_metrics = {'precision': 0, 'recall': 0, 'f1_score': 0}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x.to(device), data.edge_index.to(device))\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask].to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model(data.x.to(device), data.edge_index.to(device))\n",
        "            val_loss = criterion(out[data.val_mask], data.y[data.val_mask].to(device))\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            preds = torch.argmax(out[data.test_mask], dim=1)\n",
        "            labels = data.y[data.test_mask].to(device)\n",
        "            precision, recall, f1_score, _ = precision_recall_fscore_support(labels.cpu(), preds.cpu(), average='macro')\n",
        "\n",
        "            # Update best metrics\n",
        "            if f1_score > best_metrics['f1_score']:\n",
        "                best_metrics.update({'precision': precision, 'recall': recall, 'f1_score': f1_score})\n",
        "\n",
        "    return train_losses, val_losses, best_metrics\n",
        "\n",
        "def evaluate_predictions(probs, labels):\n",
        "    epsilon = 1e-9  # Small constant to prevent log(0)\n",
        "    probs_clamped = torch.clamp(probs, epsilon, 1-epsilon)\n",
        "    log_probs = torch.log(probs_clamped)\n",
        "    loss_function = torch.nn.NLLLoss()\n",
        "    log_loss = loss_function(log_probs, labels)\n",
        "    return log_loss.item()\n",
        "\n",
        "def plot_losses(train_losses, val_losses, best_metrics, title):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss', linestyle='--')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.ylim(0, max(max(train_losses), max(val_losses)) + 0.05)  # Adjust y-limit\n",
        "    plt.legend()\n",
        "    # plt.text(0.5, 0.1, f'Best Precision: {best_metrics[\"precision\"]:.4f}\\n'\n",
        "    #                    f'Best Recall: {best_metrics[\"recall\"]:.4f}\\n'\n",
        "    #                    f'Best F1 Score: {best_metrics[\"f1_score\"]:.4f}', horizontalalignment='center',\n",
        "    #          verticalalignment='center', transform=plt.gca().transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.5))\n",
        "    plt.show()\n",
        "\n",
        "# Initialize and train models as before, then plot with the new `plot_losses` function including best metrics annotations.\n",
        "\n",
        "# Initialize models with 50% dropout\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_2_layers = GCNWithCustomDropout(subset_graph.num_features, 64, dataset.num_classes, 2, dropout_rate=.5, aggr= SoftmaxAggregation()).to(device)\n",
        "model_10_layers = GCNWithCustomDropout(subset_graph.num_features, 64, dataset.num_classes, 2, dropout_rate=0.5, aggr=['max', 'sum', 'mean']).to(device)\n",
        "model_50_layers = GCNWithCustomDropout(subset_graph.num_features, 64, dataset.num_classes, 2, dropout_rate=0.50, aggr=[SoftmaxAggregation(), StdAggregation() ] ).to(device)\n",
        "\n",
        "# Create optimizers and criterion\n",
        "optimizer_2 = torch.optim.Adam(model_2_layers.parameters(), lr=0.005)\n",
        "optimizer_10 = torch.optim.Adam(model_10_layers.parameters(), lr=0.005)\n",
        "optimizer_50 = torch.optim.Adam(model_50_layers.parameters(), lr=0.005)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and evaluate each model\n",
        "train_losses_2, val_losses_2, best_metrics_2 = train_and_evaluate(model_2_layers, subset_graph, device, optimizer_2, criterion, 12000)\n",
        "train_losses_10, val_losses_10 , best_metrics_10= train_and_evaluate(model_10_layers, subset_graph, device, optimizer_10, criterion, 12000)\n",
        "train_losses_50, val_losses_50 , best_metrics_50 = train_and_evaluate(model_50_layers, subset_graph, device, optimizer_50, criterion, 12000)\n",
        "\n",
        "# Plot the losses with aligned axes\n",
        "max_loss = max(max(train_losses_2 + val_losses_2), max(train_losses_10 + val_losses_10), max(train_losses_50 + val_losses_50))\n",
        "\n",
        "\n",
        "# Plot the losses\n",
        "plot_losses(train_losses_2, val_losses_2, best_metrics_2, \"Training and Validation Losses for 2 Layers with 50% Dropout\")\n",
        "plot_losses(train_losses_10, val_losses_10, best_metrics_10, \"Training and Validation Losses for 10 Layers with 50% Dropout\")\n",
        "plot_losses(train_losses_50, val_losses_50, best_metrics_50, \"Training and Validation Losses for 50 Layers with 50% Dropout\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4koztywaD2F7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_model(model, data, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(data.x.to(device), data.edge_index.to(device))\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        preds = torch.argmax(probs, dim=1)\n",
        "        return preds, data.y.to(device), probs\n",
        "\n",
        "def evaluate_predictions(probs, labels):\n",
        "    epsilon = 1e-9  # Small constant to prevent log(0)\n",
        "    probs_clamped = torch.clamp(probs, epsilon, 1-epsilon)\n",
        "    log_probs = torch.log(probs_clamped)\n",
        "    loss_function = torch.nn.NLLLoss()\n",
        "    log_loss = loss_function(log_probs, labels)\n",
        "    return log_loss.item()\n",
        "\n",
        "\n",
        "# Testing and evaluation of models\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "preds_2, labels_2, probs_2 = test_model(model_2_layers, subset_graph, device)\n",
        "preds_10, labels_10, probs_10 = test_model(model_10_layers, subset_graph, device)\n",
        "preds_50, labels_50, probs_50 = test_model(model_50_layers, subset_graph, device)\n",
        "\n",
        "log_loss_2 = evaluate_predictions(probs_2, labels_2)\n",
        "log_loss_10 = evaluate_predictions(probs_10, labels_10)\n",
        "log_loss_50 = evaluate_predictions(probs_50, labels_50)\n",
        "\n",
        "# Compute metrics for each model\n",
        "precision_2 = precision_score(labels_2.cpu().numpy(), preds_2.cpu().numpy(), average='weighted')\n",
        "recall_2 = recall_score(labels_2.cpu().numpy(), preds_2.cpu().numpy(), average='weighted')\n",
        "f1_2 = f1_score(labels_2.cpu().numpy(), preds_2.cpu().numpy(), average='weighted')\n",
        "cm_2 = confusion_matrix(labels_2.cpu().numpy(), preds_2.cpu().numpy())\n",
        "\n",
        "precision_10 = precision_score(labels_10.cpu().numpy(), preds_10.cpu().numpy(), average='weighted')\n",
        "recall_10 = recall_score(labels_10.cpu().numpy(), preds_10.cpu().numpy(), average='weighted')\n",
        "f1_10 = f1_score(labels_10.cpu().numpy(), preds_10.cpu().numpy(), average='weighted')\n",
        "cm_10 = confusion_matrix(labels_10.cpu().numpy(), preds_10.cpu().numpy())\n",
        "\n",
        "precision_50 = precision_score(labels_50.cpu().numpy(), preds_50.cpu().numpy(), average='weighted')\n",
        "recall_50 = recall_score(labels_50.cpu().numpy(), preds_50.cpu().numpy(), average='weighted')\n",
        "f1_50 = f1_score(labels_50.cpu().numpy(), preds_50.cpu().numpy(), average='weighted')\n",
        "cm_50 = confusion_matrix(labels_50.cpu().numpy(), preds_50.cpu().numpy())\n",
        "\n",
        "# Print metrics for each model\n",
        "print(\"2-Layer:\")\n",
        "print(f\"Precision: {precision_2:.4f}, Recall: {recall_2:.4f}, F1 Score: {f1_2:.4f}, Log Loss: {log_loss_2:.4f}\")\n",
        "sns.heatmap(cm_2, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix - 0% Dropout')\n",
        "plt.show()\n",
        "\n",
        "print(\"10-Layer:\")\n",
        "print(f\"Precision: {precision_10:.4f}, Recall: {recall_10:.4f}, F1 Score: {f1_10:.4f}, Log Loss: {log_loss_10:.4f}\")\n",
        "sns.heatmap(cm_10, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix - 50% Dropout')\n",
        "plt.show()\n",
        "\n",
        "print(\"50-Layer:\")\n",
        "print(f\"Precision: {precision_50:.4f}, Recall: {recall_50:.4f}, F1 Score: {f1_50:.4f}, Log Loss: {log_loss_50:.4f}\")\n",
        "sns.heatmap(cm_50, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title('Confusion Matrix - 85% Dropout')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Hh0sXBFD6iA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained models (assuming they are already trained and loaded into gcn_model and graphsage_model)\n",
        "gcn_model.eval()\n",
        "graphsage_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = gcn_model(subset_graph.x, subset_graph.edge_index, return_embeds=True)\n"
      ],
      "metadata": {
        "id": "Gjua2GUji38R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compute cosine similarity matrices\n",
        "similarity_matrix = cosine_similarity(embeddings.cpu().numpy())\n",
        "\n",
        "# Example usage: Find the top-k most similar items to a given product node\n",
        "product_idx = 123  # Replace with the index of the product node\n",
        "top_k = 6\n",
        "\n",
        "# For GCN embeddings\n",
        "top_k_similar_indices = np.argsort(-similarity_matrix[product_idx])[:top_k]\n",
        "print(f\"Top {top_k} similar products to product {product_idx} according to GCN:\")\n",
        "print(top_k_similar_indices)"
      ],
      "metadata": {
        "id": "3WeB2CRdjKMF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}