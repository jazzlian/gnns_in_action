{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED83S47iIAaZ"
      },
      "source": [
        "**ETL and Preprocessing from Chapter 2 of GNNs in Action.**\n",
        "\n",
        "This notebook will start with raw data files and:\n",
        "1. Create a graph in Edge List and Adjacency List format, saved into text files.\n",
        "2. Load this graph into NetworkX using the adjacency list.\n",
        "3. Use NetworkX for light EDA and visualization.\n",
        "4. Use Pytorch Geometric (PyG) to pre-process the graph into a PyG dataset that can be used for training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0WGiL8MH33g"
      },
      "source": [
        "I. Get raw data (json and csv files) from shared google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asMRvd-u19K6",
        "outputId": "087478cf-fb26-46b9-a56c-360da11cb6a8"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1xITW9zRpkzi0tuebj2e9GIFa5jvokbqF # download sample json file\n",
        "!gdown https://drive.google.com/uc?id=1gL63dwzlfjSfBHl8b-JRjBnjyRBhbHAZ # download sample csv file\n",
        "\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xITW9zRpkzi0tuebj2e9GIFa5jvokbqF\n",
            "To: /content/relationships_hashed.json\n",
            "100% 964k/964k [00:00<00:00, 11.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gL63dwzlfjSfBHl8b-JRjBnjyRBhbHAZ\n",
            "To: /content/node_attributes_hashed.csv\n",
            "100% 155k/155k [00:00<00:00, 3.53MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prxYw6tg2-7s"
      },
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "import json\n",
        "from multiprocessing import Pool\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVu35L7g6JuT"
      },
      "source": [
        "II. Load data files into memory.\n",
        "\n",
        "First, turn the csv file of node properties into a Panda's Dataframe, then into a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzMglkiEd1KW"
      },
      "source": [
        "node_attr = pd.read_csv('/content/node_attributes_hashed.csv', encoding = \"ISO-8859-1\")\n",
        "node_attr = node_attr[['hashedid','company_type','position_type']]\n",
        "node_attr.info()\n",
        "\n",
        "node_attr = node_attr.set_index('hashedid')\n",
        "# node_attr.tail()\n",
        "attribute_dict = node_attr.to_dict(orient='index')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI5rEKXJg_TK"
      },
      "source": [
        "# attribute_dict"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyxG3TGuHwFS"
      },
      "source": [
        "Read JSON File."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR3rB1J23Vzw"
      },
      "source": [
        "# Opening JSON file\n",
        "candidate_link_file = open('relationships_hashed.json')\n",
        "\n",
        "# returns JSON object as a dictionary\n",
        "adjacencies_from_candidate_referrals = json.load(candidate_link_file)\n",
        "\n",
        "# Closing file\n",
        "candidate_link_file.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0zqZfvKt0VF"
      },
      "source": [
        "len(set(adjacencies_from_candidate_referrals.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOe92XEAWUd1"
      },
      "source": [
        "III. Create Edge List and Adjacency List."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_d3usbWBEV9"
      },
      "source": [
        "def create_adjacency_list(data_dict, suffix=''):\n",
        "    '''\n",
        "    This function is meant to illustrate the transformation of raw\n",
        "    data into an adjacency list. It was created for the social graph\n",
        "    use case.\n",
        "\n",
        "    INPUT: a. a dictionary of candidate referrals where they keys\n",
        "              are members who have referred other candidates, and\n",
        "              the values are lists of the people who where referred.\n",
        "           b. a suffix to append to the file name\n",
        "\n",
        "    OUTPUT: i. An encoded adjacency list in a txt file.\n",
        "            ii. A list of the node IDs found.\n",
        "    '''\n",
        "    list_of_nodes = []\n",
        "\n",
        "    for source_node in list(data_dict.keys()):\n",
        "\n",
        "        if source_node not in list_of_nodes:\n",
        "            list_of_nodes.append(source_node)\n",
        "\n",
        "        for y in data_dict[source_node]:\n",
        "\n",
        "            if source_node not in list_of_nodes:\n",
        "                list_of_nodes.append(y)\n",
        "            if y not in data_dict.keys():\n",
        "                data_dict[y]=[source_node]\n",
        "\n",
        "            else:\n",
        "                if source_node not in data_dict[y]:\n",
        "                    data_dict[y].append(source_node)\n",
        "\n",
        "                else: continue\n",
        "\n",
        "\n",
        "    g= open(\"adjacency_list_{}.txt\".format(suffix),\"w+\")\n",
        "\n",
        "    for source_node in list(data_dict.keys()):\n",
        "        dt = ' '.join(data_dict[source_node])\n",
        "        # print(\"{} {}\".format(source_node, dt))\n",
        "        g.write(\"{} {} \\n\".format(source_node, dt))\n",
        "\n",
        "    g.close\n",
        "\n",
        "    print(len(list_of_nodes))\n",
        "    return list_of_nodes, data_dict"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29Ukwh6LBOxu"
      },
      "source": [
        "list_of_nodes_adj, candidate_dict = create_adjacency_list(adjacencies_from_candidate_referrals, 'candidates')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVHKSXbasPEc"
      },
      "source": [
        "# list_of_nodes_adj"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtXRML4lhri3"
      },
      "source": [
        "graph_from_adj = nx.read_adjlist('adjacency_list_candidates.txt')\n",
        "graph_from_adj.number_of_edges(), graph_from_adj.number_of_nodes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LQ3_nA6BjIi"
      },
      "source": [
        "# create edge list\n",
        "\n",
        "def create_edge_list(data_dict, suffix=''):\n",
        "    '''\n",
        "    This function is meant to illustrate the transformation of raw\n",
        "    data into an edge list. It was created for the social graph\n",
        "    use case.\n",
        "\n",
        "    INPUT: a. a dictionary of candidate referrals where they keys\n",
        "              are members who have referred other candiates, and\n",
        "              the values are lists of the people who where referred.\n",
        "           b. a suffix to append to the file name\n",
        "\n",
        "    OUTPUT: i. An edge adjacency list in a txt file.\n",
        "            ii. Lists of the node IDs found and the edges generated.\n",
        "    '''\n",
        "\n",
        "\n",
        "    edge_list_file = open(\"edge_list_{}.txt\".format(suffix),\"w+\")\n",
        "    list_of_edges = []\n",
        "    list_of_nodes_all = []\n",
        "\n",
        "    for source_node in list(data_dict.keys()):\n",
        "        if source_node not in list_of_nodes_all:\n",
        "            list_of_nodes_all.append(source_node)\n",
        "        list_of_connects = data_dict[source_node]\n",
        "\n",
        "        for destination_node in list_of_connects:\n",
        "            if destination_node not in list_of_nodes_all:\n",
        "                list_of_nodes_all.append(destination_node)\n",
        "\n",
        "            if {source_node, destination_node} not in list_of_edges:\n",
        "                print(\"{} {}\".format(source_node, destination_node))\n",
        "                edge_list_file.write(\"{} {} \\n\".format(source_node, destination_node))\n",
        "                list_of_edges.append({source_node, destination_node})\n",
        "\n",
        "            else: continue\n",
        "\n",
        "    edge_list_file.close\n",
        "    return list_of_edges, list_of_nodes_all"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGLcXFglHFIf"
      },
      "source": [
        "list_of_edges, list_of_nodes = create_edge_list(adjacencies_from_candidate_referrals, 'candidates')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYt870rxmNEM"
      },
      "source": [
        "len(list_of_edges), len(list_of_nodes), len(list_of_nodes_adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRsf2u3aiIxR"
      },
      "source": [
        "graph_from_edge_list = nx.read_edgelist('edge_list_candidates.txt')\n",
        "graph_from_edge_list.number_of_edges(), graph_from_edge_list.number_of_nodes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TzugkbnHshq"
      },
      "source": [
        "IV. Data Exploration\n",
        "In this section, we:\n",
        "\n",
        "\n",
        "*   load our adjacency list into NetworkX\n",
        "*   Establish the number of nodes, edges and connected components\n",
        "*   Select the large connected component for some visualizations and summary statistics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM70RHjiHSzm"
      },
      "source": [
        "social_graph = nx.read_edgelist('edge_list_candidates.txt')\n",
        "nx.set_node_attributes(social_graph, attribute_dict)\n",
        "print(social_graph.number_of_nodes(), social_graph.number_of_edges())\n",
        "# number of nodes and edges"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "social_graph = nx.read_adjlist('adjacency_list_candidates.txt')\n",
        "nx.set_node_attributes(social_graph, attribute_dict)\n",
        "print(social_graph.number_of_nodes(), social_graph.number_of_edges())"
      ],
      "metadata": {
        "id": "gR7unOWl2tVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi8HpuUhI8wP"
      },
      "source": [
        "len(list((c for c in nx.connected_components(social_graph))))\n",
        "# There are 219 connected components"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjx9X0lfezxP"
      },
      "source": [
        "sorted_components = sorted(list((len(c) for c in nx.connected_components(social_graph))), reverse=True)\n",
        "set(sorted_components)\n",
        "# We are able to determine that most of the disconnected components are made up of less\n",
        "# than 4 nodes. Our interest is on the large component of 1698 nodes.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRwvYwyrb4Hz"
      },
      "source": [
        "# We use the 'job type' attribute to create groups that we\n",
        "# can use in our visualizations.\n",
        "\n",
        "\n",
        "groups = set(nx.get_node_attributes(social_graph,'position_type').values())\n",
        "mapping = dict(zip(sorted(groups),count()))\n",
        "values = [mapping[attribute_dict[node]['position_type']] if node in attribute_dict.keys() else .25 \\\n",
        "          for node in social_graph.nodes() ]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C57W6MgmKOe"
      },
      "source": [
        "mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-w4elXl0GXW"
      },
      "source": [
        "This below visualization is of the entire graph, including the disconnected components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtwd24xqpT6S"
      },
      "source": [
        "nx.draw(social_graph, cmap=plt.get_cmap('viridis'), node_color=values, node_size=20,with_labels=False, font_color='white')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn8DgKgz0K52"
      },
      "source": [
        "Below is a recipe for visualizing and performing summary stats, modified from NetworkX documentation.\n",
        "\n",
        "Some highlights:\n",
        "* #A This command creates a distinct graph object from the  largest connected component of a given graph. In our example there is only one connected component, so the way this command is programmed is a bit of overkill.\n",
        "* #B Determines how exactly the nodes and edges will be positioned in the visualization. NetworkX includes a few algorithms to choose from. Spring Layout follows an algorithm that models edges as springs, and nodes as masses that repel one another.\n",
        "* #C Draws the nodes according to the layout given in the previous line. The draw_network_nodes method has a few input parameters, including the node_side parameter, which adjusts the size of the nodes in the visualization.\n",
        "* #D Draws the edges according to the layout given above. Like the draw_network_nodes method, this method has multiple ways to specify the appearance of the nodes. The alpha parameter specifies the transparency of the edges.\n",
        "* #E Graph objects in NetworkX have various methods and attributes. In this line, we use the degree method to generate an iterable of nodes with their respective degrees. This is nested in a sorted comprehension whose output is a list of degrees sorted highest to lowest.\n",
        "* #F We plot the list generated in #E\n",
        "* #G We plot a histogram of the number of degrees of the nodes of the graph. Numpy’s unique method (with the return_counts parameter) generates two lists: the nodes’ degrees, and their respective counts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjLNdkvNJAYR"
      },
      "source": [
        "\n",
        "\n",
        "fig = plt.figure(\"Degree of a random graph\", figsize=(8, 8))\n",
        "# Create a gridspec for adding subplots of different sizes\n",
        "axgrid = fig.add_gridspec(5, 4)\n",
        "\n",
        "ax0 = fig.add_subplot(axgrid[0:3, :])\n",
        "\n",
        "Gcc = social_graph.subgraph(sorted(nx.connected_components(social_graph), key=len, reverse=True)[0])  #A\n",
        "\n",
        "groups2 = set(nx.get_node_attributes(Gcc,'position_type').values())\n",
        "nx.get_node_attributes(Gcc,'position_type')\n",
        "mapping2 = dict(zip(sorted(groups2),count()))\n",
        "values2 = [mapping2[attribute_dict[node]['position_type']] for node in Gcc.nodes()]\n",
        "\n",
        "\n",
        "pos = nx.spring_layout(Gcc, seed=10396953)  #B\n",
        "nx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20, node_color=values2) #C\n",
        "nx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4) #D\n",
        "ax0.set_title(\"Connected component of Social Graph\")\n",
        "ax0.set_axis_off()\n",
        "\n",
        "degree_sequence = sorted([d for n, d in social_graph.degree()], reverse=True) #E\n",
        "\n",
        "ax1 = fig.add_subplot(axgrid[3:, :2])\n",
        "ax1.plot(degree_sequence, \"b-\", marker=\"o\") #F\n",
        "ax1.set_title(\"Degree Rank Plot\")\n",
        "ax1.set_ylabel(\"Degree\")\n",
        "ax1.set_xlabel(\"Rank\")\n",
        "\n",
        "ax2 = fig.add_subplot(axgrid[3:, 2:])\n",
        "ax2.bar(*np.unique(degree_sequence, return_counts=True)) #G\n",
        "ax2.set_title(\"Degree histogram\")\n",
        "ax2.set_xlabel(\"Degree\")\n",
        "ax2.set_ylabel(\"# of Nodes\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_JXYkze2le-"
      },
      "source": [
        "The number of nodes and edges in our large connected component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvB2N3NuATDw"
      },
      "source": [
        "Gcc.number_of_edges(), Gcc.number_of_nodes()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eedIp3CUz_xB"
      },
      "source": [
        "A (sparse) adjacency matrix of our entire social graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwj6oc5YJJN5"
      },
      "source": [
        "plt.imshow(nx.to_numpy_array(social_graph), aspect='equal',cmap='hot')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKgMhG96J1GP"
      },
      "source": [
        "V. Preprocessing with Pytorch Geometric\n",
        "\n",
        "Below, we show a few methods to loading data into pytorch geometric for training:\n",
        "\n",
        "\n",
        "\n",
        "*   Create directly from a NetworkX graph object\n",
        "*   Using raw files.\n",
        "*   Using Dataset classes with raw files\n",
        "*   Using Data objects to directly create a dataloader without the Dataset class\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVNXeHelRXG-"
      },
      "source": [
        "! python -c \"import torch; print(torch.__version__)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ANFI3PiRc0E"
      },
      "source": [
        "! python -c \"import torch; print(torch.version.cuda)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmgeBAvjKe10"
      },
      "source": [
        "# Install Pytorch Geometric\n",
        "# Use the information above to fill in the first and second pip lines\n",
        "\n",
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
        "!pip install -q torch-geometric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EroFT1ZJZzZ"
      },
      "source": [
        "import torch\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import InMemoryDataset\n",
        "# from torch_geometric import utils\n",
        "from torch_geometric.utils.convert import to_networkx, from_networkx\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwGfBastzzOF"
      },
      "source": [
        "Case A: Create PyG data object using NetworkX object\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LIXBWIWJ5rX"
      },
      "source": [
        "\n",
        "data = from_networkx(Gcc)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9a1iihJz0F0"
      },
      "source": [
        "Case B: Create PyG data object using raw files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE3od-vDMleb"
      },
      "source": [
        "\n",
        "social_graph = nx.read_adjlist('adjacency_list_candidates.txt')  #A\n",
        "\n",
        "list_of_nodes = list(set(list(social_graph))) #B\n",
        "indices_of_nodes = [list_of_nodes.index(x) for x in list_of_nodes]  #C\n",
        "\n",
        "node_to_index = dict(zip(list_of_nodes, indices_of_nodes))  #D\n",
        "index_to_node = dict(zip(indices_of_nodes, list_of_nodes))\n",
        "\n",
        "list_edges = nx.convert.to_edgelist(social_graph)  #E\n",
        "list_edges = list(list_edges)\n",
        "named_edge_list_0 = [x[0] for x in list_edges] #F\n",
        "named_edge_list_1 = [x[1] for x in list_edges]\n",
        "\n",
        "indexed_edge_list_0 = [node_to_index[x] for x in named_edge_list_0]  #G\n",
        "indexed_edge_list_1 = [node_to_index[x] for x in named_edge_list_1]\n",
        "\n",
        "x = torch.FloatTensor([[1] for x in range(len(list_of_nodes))])#  [[] for x in xrange(n)]  #H\n",
        "y = torch.FloatTensor([1]*974 + [0]*973) #I\n",
        "y = y.long()\n",
        "\n",
        "edge_index = torch.tensor([indexed_edge_list_0, indexed_edge_list_1])  #J\n",
        "\n",
        "train_mask = torch.zeros(len(list_of_nodes), dtype=torch.uint8) #K\n",
        "train_mask[:int(0.8 * len(list_of_nodes))] = 1 #train only on the 80% nodes\n",
        "test_mask = torch.zeros(len(list_of_nodes), dtype=torch.uint8) #test on 20 % nodes\n",
        "test_mask[- int(0.2 * len(list_of_nodes)):] = 1\n",
        "train_mask = train_mask.bool()\n",
        "test_mask = test_mask.bool()\n",
        "\n",
        "data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, test_mask=test_mask)  #L\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXvFan9yM1I1"
      },
      "source": [
        "Case C: Create PyG dataset object using custom class and input files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-pJLIiLMvyo"
      },
      "source": [
        "class MyOwnDataset(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):  #A\n",
        "        super(MyOwnDataset, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self): #B\n",
        "        return []\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self): #C\n",
        "        return ['../test.dataset']\n",
        "\n",
        "    def download(self): #D\n",
        "        # Download to `self.raw_dir`.\n",
        "        pass\n",
        "\n",
        "    def process(self): #E\n",
        "        # Read data into `Data` list.\n",
        "        data_list = []\n",
        "\n",
        "        eg = nx.read_edgelist('edge_list2.txt')\n",
        "\n",
        "        list_of_nodes = list(set(list(eg)))\n",
        "        indices_of_nodes = [list_of_nodes.index(x) for x in list_of_nodes]\n",
        "\n",
        "        node_to_index = dict(zip(list_of_nodes, indices_of_nodes))\n",
        "        index_to_node = dict(zip(indices_of_nodes, list_of_nodes))\n",
        "\n",
        "        list_edges = nx.convert.to_edgelist(eg)\n",
        "        list_edges = list(list_edges)\n",
        "        named_edge_list_0 = [x[0] for x in list_edges]\n",
        "        named_edge_list_1 = [x[1] for x in list_edges]\n",
        "\n",
        "        indexed_edge_list_0 = [node_to_index[x] for x in named_edge_list_0]\n",
        "        indexed_edge_list_1 = [node_to_index[x] for x in named_edge_list_1]\n",
        "\n",
        "        x = torch.FloatTensor([[1] for x in range(len(list_of_nodes))])#  [[] for x in xrange(n)]\n",
        "        y = torch.FloatTensor([1]*974 + [0]*973)\n",
        "        y = y.long()\n",
        "\n",
        "        edge_index = torch.tensor([indexed_edge_list_0, indexed_edge_list_1])\n",
        "\n",
        "        train_mask = torch.zeros(len(list_of_nodes), dtype=torch.uint8)\n",
        "        train_mask[:int(0.8 * len(list_of_nodes))] = 1 #train only on the 80% nodes\n",
        "        test_mask = torch.zeros(len(list_of_nodes), dtype=torch.uint8) #test on 20 % nodes\n",
        "        test_mask[- int(0.2 * len(list_of_nodes)):] = 1\n",
        "\n",
        "        train_mask = train_mask.bool()\n",
        "\n",
        "        test_mask = test_mask.bool()\n",
        "\n",
        "        data_example = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, test_mask=test_mask) #F\n",
        "\n",
        "        data_list.append(data_example) #G\n",
        "\n",
        "        data, slices = self.collate(data_list)  #H\n",
        "        torch.save((data, slices), self.processed_paths[0])  #I"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtSvMpANzufv"
      },
      "source": [
        "Case D: Create PyG data objects for use in dataloader without use of a dataset object\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTItD1H9NOBe"
      },
      "source": [
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# data_list = [Data(...), ..., Data(...)]\n",
        "# We'll use the data object from case B\n",
        "data_list = [data]\n",
        "loader = DataLoader(data_list, batch_size=32)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aG2rElm2MBCp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}