{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED83S47iIAaZ"
      },
      "source": [
        "## ETL and Preprocessing from Chapter 2 of GNNs in Action\n",
        "\n",
        "## I. Introduction and Setup\n",
        "\n",
        "This notebook covers the ETL and preprocessing steps for Chapter 2 of GNNs in Action. We’ll walk through creating a graph, loading it into NetworkX, performing light EDA, and finally preprocessing the graph into a PyG dataset for training.\n",
        "\n",
        "This notebook covers the steps to:\n",
        "1. Create a graph in both Edge List and Adjacency List formats, saving them into text files.\n",
        "2. Load the graph into NetworkX using the adjacency list.\n",
        "3. Perform light Exploratory Data Analysis (EDA) and visualization with NetworkX.\n",
        "4. Pre-process the graph into a Pytorch Geometric (PyG) dataset for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0WGiL8MH33g"
      },
      "source": [
        "## II. Download Raw Data\n",
        "We begin by downloading raw data files, which include JSON and CSV files, from a shared Google Drive link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asMRvd-u19K6",
        "outputId": "087478cf-fb26-46b9-a56c-360da11cb6a8"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1xITW9zRpkzi0tuebj2e9GIFa5jvokbqF # download sample json file\n",
        "!gdown https://drive.google.com/uc?id=1gL63dwzlfjSfBHl8b-JRjBnjyRBhbHAZ # download sample csv file"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xITW9zRpkzi0tuebj2e9GIFa5jvokbqF\n",
            "To: /content/relationships_hashed.json\n",
            "100% 964k/964k [00:00<00:00, 11.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1gL63dwzlfjSfBHl8b-JRjBnjyRBhbHAZ\n",
            "To: /content/node_attributes_hashed.csv\n",
            "100% 155k/155k [00:00<00:00, 3.53MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. Import Libraries and Load Data\n",
        "Next, we’ll import the necessary libraries and load the data into memory."
      ],
      "metadata": {
        "id": "gbIW3rDtPEz6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prxYw6tg2-7s"
      },
      "source": [
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "import json\n",
        "from multiprocessing import Pool\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVu35L7g6JuT"
      },
      "source": [
        "## IV. Load Node Attributes\n",
        "We’re loading the CSV file of node properties into a Pandas DataFrame and then converting it into a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzMglkiEd1KW"
      },
      "source": [
        "node_attr = pd.read_csv('/content/node_attributes_hashed.csv', encoding = \"ISO-8859-1\")\n",
        "node_attr = node_attr[['hashedid','company_type','position_type']]\n",
        "node_attr.info()\n",
        "\n",
        "node_attr = node_attr.set_index('hashedid')\n",
        "# node_attr.tail()\n",
        "attribute_dict = node_attr.to_dict(orient='index')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI5rEKXJg_TK"
      },
      "source": [
        "# attribute_dict"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyxG3TGuHwFS"
      },
      "source": [
        "## V. Load JSON File\n",
        "This part is for loading a JSON file which contains information about the relationships between nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rR3rB1J23Vzw"
      },
      "source": [
        "# Opening JSON file\n",
        "candidate_link_file = open('relationships_hashed.json')\n",
        "\n",
        "# returns JSON object as a dictionary\n",
        "adjacencies_from_candidate_referrals = json.load(candidate_link_file)\n",
        "\n",
        "# Closing file\n",
        "candidate_link_file.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0zqZfvKt0VF"
      },
      "source": [
        "len(set(adjacencies_from_candidate_referrals.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOe92XEAWUd1"
      },
      "source": [
        "## VI. Create Edge List and Adjacency List\n",
        "This section is where we create both edge and adjacency lists from the loaded JSON file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adjacency_list(data_dict, suffix=''):\n",
        "    '''\n",
        "    This function is meant to illustrate the transformation of raw\n",
        "    data into an adjacency list. It was created for the social graph\n",
        "    use case.\n",
        "\n",
        "    INPUT: a. a dictionary of candidate referrals where they keys\n",
        "              are members who have referred other candidates, and\n",
        "              the values are lists of the people who where referred.\n",
        "           b. a suffix to append to the file name\n",
        "\n",
        "    OUTPUT: i. An encoded adjacency list in a txt file.\n",
        "            ii. A list of the node IDs found.\n",
        "    '''\n",
        "\n",
        "    # Initialize an empty list to store unique nodes\n",
        "    list_of_nodes = []\n",
        "\n",
        "    # Iterate over all keys (source nodes) in the data dictionary\n",
        "    for source_node in data_dict.keys():\n",
        "\n",
        "        # Add the source node to the list of nodes if it is not already present\n",
        "        if source_node not in list_of_nodes:\n",
        "            list_of_nodes.append(source_node)\n",
        "\n",
        "        # Iterate over all nodes referred by the source node\n",
        "        for y in data_dict[source_node]:\n",
        "\n",
        "            # Add the referred node to the list of nodes if it is not already present\n",
        "            if y not in list_of_nodes:\n",
        "                list_of_nodes.append(y)\n",
        "\n",
        "            # If the referred node is not already a key in the dictionary, add it and\n",
        "            # create a connection back to the source node\n",
        "            if y not in data_dict:\n",
        "                data_dict[y] = [source_node]\n",
        "            # Otherwise, just add a connection back to the source node if it doesn't exist\n",
        "            elif source_node not in data_dict[y]:\n",
        "                data_dict[y].append(source_node)\n",
        "\n",
        "    # Open a file with the given suffix to write the adjacency list\n",
        "    with open(\"adjacency_list_{}.txt\".format(suffix), \"w+\") as g:\n",
        "\n",
        "        # Iterate over all keys (nodes) in the updated dictionary\n",
        "        for source_node in data_dict.keys():\n",
        "\n",
        "            # Join the list of adjacent nodes into a string\n",
        "            dt = ' '.join(data_dict[source_node])\n",
        "\n",
        "            # Write the node and its adjacent nodes to the file\n",
        "            g.write(\"{} {}\\n\".format(source_node, dt))\n",
        "\n",
        "    # Print the total number of unique nodes found\n",
        "    print(len(list_of_nodes))\n",
        "\n",
        "    # Create an adjacency list by joining each list of adjacent nodes into a string\n",
        "    adjacency_list = {k: ' '.join(v) for k, v in data_dict.items()}\n",
        "\n",
        "    # Return the list of nodes and the adjacency list\n",
        "    return list_of_nodes, adjacency_list\n"
      ],
      "metadata": {
        "id": "L6eqJbntW8zJ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29Ukwh6LBOxu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ffb3db-ba13-4bb9-bc2e-dc748d6058bd"
      },
      "source": [
        "list_of_nodes_adj, candidate_dict = create_adjacency_list(adjacencies_from_candidate_referrals, 'candidates')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVHKSXbasPEc"
      },
      "source": [
        "# list_of_nodes_adj"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtXRML4lhri3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ebf8a69-281b-45d7-fa9a-a457c0a09bb3"
      },
      "source": [
        "graph_from_adj = nx.read_adjlist('adjacency_list_candidates.txt')\n",
        "graph_from_adj.number_of_edges(), graph_from_adj.number_of_nodes()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12239, 1933)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creation of Edge List**\n",
        "*   The create_edge_list function takes in a dictionary of nodes and their connections and transforms it into an edge list.\n",
        "*   It iteratively goes through each node and its connections, and for each unique pair, it adds them to an edge list.\n",
        "*   This edge list is then written to a file for future use.\n",
        "*   All unique nodes and edges are also returned from the function for additional processing or analysis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MzKE9hlwRHkB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LQ3_nA6BjIi"
      },
      "source": [
        "# create edge list\n",
        "\n",
        "def create_edge_list(data_dict, suffix=''):\n",
        "    '''\n",
        "    This function transforms a dictionary of nodes and their connections into an edge list and saves it to a file.\n",
        "\n",
        "    INPUT:\n",
        "      - data_dict: a dictionary where keys are nodes and values are lists of nodes that are connected to the key node.\n",
        "      - suffix: a string to append to the end of the file name for the saved edge list.\n",
        "\n",
        "    OUTPUT:\n",
        "      - A text file containing the edge list.\n",
        "      - A list of unique nodes found in the edge list.\n",
        "      - A list of edges found in the edge list.\n",
        "    '''\n",
        "    # Opening a file to write the edge list. The file name is generated by appending the provided suffix.\n",
        "    edge_list_file = open(\"edge_list_{}.txt\".format(suffix), \"w+\")\n",
        "\n",
        "    # A list to store the edges as they are found.\n",
        "    list_of_edges = []\n",
        "\n",
        "    # A list to store all the nodes found in the data_dict.\n",
        "    list_of_nodes_all = []\n",
        "\n",
        "    # Iterating over each node in the data_dict.\n",
        "    for source_node in list(data_dict.keys()):\n",
        "        # If the source_node is not already in the list_of_nodes_all, add it.\n",
        "        if source_node not in list_of_nodes_all:\n",
        "            list_of_nodes_all.append(source_node)\n",
        "\n",
        "        # Getting the list of nodes connected to the source_node.\n",
        "        list_of_connects = data_dict[source_node]\n",
        "\n",
        "        # Iterating over each node connected to the source_node.\n",
        "        for destination_node in list_of_connects:\n",
        "            # If the destination node is not already in the list_of_nodes_all, add it.\n",
        "            if destination_node not in list_of_nodes_all:\n",
        "                list_of_nodes_all.append(destination_node)\n",
        "\n",
        "            # If the edge {source_node, destination_node} is not already in the list_of_edges, add it.\n",
        "            if {source_node, destination_node} not in list_of_edges:\n",
        "                # Printing the edge to the console (this can be removed or commented out if not needed).\n",
        "                print(\"{} {}\".format(source_node, destination_node))\n",
        "\n",
        "                # Writing the edge to the edge_list_file.\n",
        "                edge_list_file.write(\"{} {} \\n\".format(source_node, destination_node))\n",
        "\n",
        "                # Adding the edge to the list_of_edges.\n",
        "                list_of_edges.append({source_node, destination_node})\n",
        "            else:  # If the edge is already in the list_of_edges, skip it and continue to the next one.\n",
        "                continue\n",
        "\n",
        "    # Closing the edge list file after writing all edges to it.\n",
        "    edge_list_file.close\n",
        "\n",
        "    # Returning the list_of_edges and list_of_nodes_all as output.\n",
        "    return list_of_edges, list_of_nodes_all\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGLcXFglHFIf"
      },
      "source": [
        "list_of_edges, list_of_nodes = create_edge_list(adjacencies_from_candidate_referrals, 'candidates')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYt870rxmNEM"
      },
      "source": [
        "len(list_of_edges), len(list_of_nodes), len(list_of_nodes_adj)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRsf2u3aiIxR"
      },
      "source": [
        "graph_from_edge_list = nx.read_edgelist('edge_list_candidates.txt')\n",
        "graph_from_edge_list.number_of_edges(), graph_from_edge_list.number_of_nodes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TzugkbnHshq"
      },
      "source": [
        "## VII. Data Exploration\n",
        "Now, we are delving into a section where we explore the data visually and statistically to gain insights.\n",
        "\n",
        "\n",
        "*   load our adjacency list into NetworkX\n",
        "*   Establish the number of nodes, edges and connected components\n",
        "*   Select the large connected component for some visualizations and summary statistics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM70RHjiHSzm"
      },
      "source": [
        "social_graph = nx.read_edgelist('edge_list_candidates.txt')\n",
        "nx.set_node_attributes(social_graph, attribute_dict)\n",
        "print(social_graph.number_of_nodes(), social_graph.number_of_edges())\n",
        "# number of nodes and edges"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "social_graph = nx.read_adjlist('adjacency_list_candidates.txt')\n",
        "nx.set_node_attributes(social_graph, attribute_dict)\n",
        "print(social_graph.number_of_nodes(), social_graph.number_of_edges())"
      ],
      "metadata": {
        "id": "gR7unOWl2tVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi8HpuUhI8wP"
      },
      "source": [
        "len(list((c for c in nx.connected_components(social_graph))))\n",
        "# There are 219 connected components"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjx9X0lfezxP"
      },
      "source": [
        "sorted_components = sorted(list((len(c) for c in nx.connected_components(social_graph))), reverse=True)\n",
        "set(sorted_components)\n",
        "# We are able to determine that most of the disconnected components are made up of less\n",
        "# than 4 nodes. Our interest is on the large component of 1698 nodes.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRwvYwyrb4Hz"
      },
      "source": [
        "# We use the 'job type' attribute to create groups that we\n",
        "# can use in our visualizations.\n",
        "\n",
        "\n",
        "groups = set(nx.get_node_attributes(social_graph,'position_type').values())\n",
        "mapping = dict(zip(sorted(groups),count()))\n",
        "values = [mapping[attribute_dict[node]['position_type']] if node in attribute_dict.keys() else .25 \\\n",
        "          for node in social_graph.nodes() ]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C57W6MgmKOe"
      },
      "source": [
        "mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-w4elXl0GXW"
      },
      "source": [
        "This below visualization is of the entire graph, including the disconnected components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtwd24xqpT6S"
      },
      "source": [
        "nx.draw(social_graph, cmap=plt.get_cmap('viridis'), node_color=values, node_size=20,with_labels=False, font_color='white')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn8DgKgz0K52"
      },
      "source": [
        "\n",
        "The below script visualizes a social graph and its degree distribution. It first extracts the largest connected\n",
        "component of the social graph and plots it with nodes colored by their 'position_type' attribute. The script\n",
        "then computes the degree of each node in the social graph and uses this information to create a degree rank\n",
        "plot and a degree histogram. The plots are arranged in a single figure using a custom grid layout.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjLNdkvNJAYR"
      },
      "source": [
        "# Create a new figure for plotting the degree of a random graph with specified figure size\n",
        "fig = plt.figure(\"Degree of a random graph\", figsize=(8, 8))\n",
        "\n",
        "# Create a grid specification of 5 rows and 4 columns to manage subplot arrangement\n",
        "axgrid = fig.add_gridspec(5, 4)\n",
        "\n",
        "# Add a subplot that occupies the first three rows and all columns of the grid\n",
        "ax0 = fig.add_subplot(axgrid[0:3, :])\n",
        "\n",
        "# Extract the largest connected component from the social graph (A)\n",
        "Gcc = social_graph.subgraph(sorted(nx.connected_components(social_graph), key=len, reverse=True)[0])\n",
        "\n",
        "# Retrieve unique values of the 'position_type' node attribute in the connected component\n",
        "groups2 = set(nx.get_node_attributes(Gcc, 'position_type').values())\n",
        "\n",
        "# Create a mapping of 'position_type' values to integers\n",
        "mapping2 = dict(zip(sorted(groups2), count()))\n",
        "\n",
        "# Obtain the integer representations of 'position_type' for all nodes in the connected component\n",
        "values2 = [mapping2[attribute_dict[node]['position_type']] for node in Gcc.nodes()]\n",
        "\n",
        "# Compute the spring layout positions for the nodes of the connected component with a given seed (B)\n",
        "pos = nx.spring_layout(Gcc, seed=10396953)\n",
        "\n",
        "# Draw nodes of the connected component using the spring layout positions and the mapped 'position_type' values as colors (C)\n",
        "nx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20, node_color=values2)\n",
        "\n",
        "# Draw edges of the connected component with specified transparency (D)\n",
        "nx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)\n",
        "\n",
        "# Set the title of the subplot and turn off the axis\n",
        "ax0.set_title(\"Connected component of Social Graph\")\n",
        "ax0.set_axis_off()\n",
        "\n",
        "# Calculate the degree sequence of the social graph, sorted in descending order (E)\n",
        "degree_sequence = sorted([d for n, d in social_graph.degree()], reverse=True)\n",
        "\n",
        "# Add another subplot to the bottom left quarter of the grid for the degree rank plot\n",
        "ax1 = fig.add_subplot(axgrid[3:, :2])\n",
        "\n",
        "# Plot the degree rank plot with blue line and circle markers (F)\n",
        "ax1.plot(degree_sequence, \"b-\", marker=\"o\")\n",
        "\n",
        "# Set the title, y-label, and x-label of the degree rank plot\n",
        "ax1.set_title(\"Degree Rank Plot\")\n",
        "ax1.set_ylabel(\"Degree\")\n",
        "ax1.set_xlabel(\"Rank\")\n",
        "\n",
        "# Add another subplot to the bottom right quarter of the grid for the degree histogram\n",
        "ax2 = fig.add_subplot(axgrid[3:, 2:])\n",
        "\n",
        "# Create a bar chart for the degree histogram (G)\n",
        "ax2.bar(*np.unique(degree_sequence, return_counts=True))\n",
        "\n",
        "# Set the title, x-label, and y-label of the degree histogram\n",
        "ax2.set_title(\"Degree histogram\")\n",
        "ax2.set_xlabel(\"Degree\")\n",
        "ax2.set_ylabel(\"# of Nodes\")\n",
        "\n",
        "# Adjust the layout to prevent overlap between subplots\n",
        "fig.tight_layout()\n",
        "\n",
        "# Display the complete figure with all subplots\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_JXYkze2le-"
      },
      "source": [
        "The number of nodes and edges in our large connected component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvB2N3NuATDw"
      },
      "source": [
        "Gcc.number_of_edges(), Gcc.number_of_nodes()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eedIp3CUz_xB"
      },
      "source": [
        "A (sparse) adjacency matrix of our entire social graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwj6oc5YJJN5"
      },
      "source": [
        "plt.imshow(nx.to_numpy_array(social_graph), aspect='equal',cmap='hot')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKgMhG96J1GP"
      },
      "source": [
        "## VIII. Preprocessing with Pytorch Geometric\n",
        "This section will guide us on how to preprocess data with Pytorch Geometric.\n",
        "\n",
        "\n",
        "\n",
        "*   Create directly from a NetworkX graph object\n",
        "*   Using raw files.\n",
        "*   Using Dataset classes with raw files\n",
        "*   Using Data objects to directly create a dataloader without the Dataset class\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVNXeHelRXG-"
      },
      "source": [
        "! python -c \"import torch; print(torch.__version__)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ANFI3PiRc0E"
      },
      "source": [
        "! python -c \"import torch; print(torch.version.cuda)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmgeBAvjKe10"
      },
      "source": [
        "# Install Pytorch Geometric\n",
        "# Use the information above to fill in the first and second pip lines\n",
        "\n",
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-2.0.1+cu118.html\n",
        "!pip install -q torch-geometric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EroFT1ZJZzZ"
      },
      "source": [
        "import torch\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import InMemoryDataset\n",
        "# from torch_geometric import utils\n",
        "from torch_geometric.utils.convert import to_networkx, from_networkx\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwGfBastzzOF"
      },
      "source": [
        "Case A: Create PyG data object using NetworkX object\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LIXBWIWJ5rX"
      },
      "source": [
        "\n",
        "data = from_networkx(Gcc)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9a1iihJz0F0"
      },
      "source": [
        "Case B: Create PyG data object using raw files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE3od-vDMleb"
      },
      "source": [
        "#A Load the adjacency list to form a graph using NetworkX.\n",
        "social_graph = nx.read_adjlist('adjacency_list_candidates.txt')\n",
        "\n",
        "#B Get all unique nodes from the graph. We use set to remove any duplicates and then convert it back to a list.\n",
        "list_of_nodes = list(set(list(social_graph)))\n",
        "\n",
        "#C Create a list of indices corresponding to each unique node. This will be used to map each node to an index.\n",
        "indices_of_nodes = [list_of_nodes.index(x) for x in list_of_nodes]\n",
        "\n",
        "#D Create a dictionary to map each node to its corresponding index.\n",
        "node_to_index = dict(zip(list_of_nodes, indices_of_nodes))\n",
        "\n",
        "# Creating the reverse mapping from index to node for easy lookup later.\n",
        "index_to_node = dict(zip(indices_of_nodes, list_of_nodes))\n",
        "\n",
        "#E Convert the graph to an edge list to extract all the edges present in the graph.\n",
        "list_edges = nx.convert.to_edgelist(social_graph)\n",
        "list_edges = list(list_edges)  # Converting edge view object to list for easier processing.\n",
        "\n",
        "#F Split the edge list into two separate lists representing the two nodes that make up each edge.\n",
        "named_edge_list_0 = [x[0] for x in list_edges]\n",
        "named_edge_list_1 = [x[1] for x in list_edges]\n",
        "\n",
        "#G Convert the node names in the edge list to their corresponding indices using the previously created mapping.\n",
        "indexed_edge_list_0 = [node_to_index[x] for x in named_edge_list_0]\n",
        "indexed_edge_list_1 = [node_to_index[x] for x in named_edge_list_1]\n",
        "\n",
        "#H Create a feature matrix x. In this example, it’s a vector of ones, meaning each node has a feature vector of size 1 with a value of 1.\n",
        "x = torch.FloatTensor([[1] for x in range(len(list_of_nodes))])\n",
        "\n",
        "#I Create a target vector y, with values of 1 for the first 974 nodes and 0 for the next 973 nodes.\n",
        "y = torch.FloatTensor([1]*974 + [0]*973)\n",
        "y = y.long()  # Convert the floating point values to long integers.\n",
        "\n",
        "#J Create an edge index tensor that contains all edges represented by the indices of the connected nodes. It's used to tell PyTorch Geometric about the connections in the graph.\n",
        "edge_index = torch.tensor([indexed_edge_list_0, indexed_edge_list_1])\n",
        "\n",
        "#K Create a training mask to identify which nodes will be used for training. It's initialized with zeros and then set to ones for nodes that are used for training.\n",
        "train_mask = torch.zeros(len(list_of_nodes), dtype=torch.uint8)\n",
        "train_mask[:int(0.8 * len(list_of_nodes))] = 1  # Use 80% of nodes for training.\n",
        "\n",
        "# Creating a test mask for identifying which nodes will be used for testing.\n",
        "test_mask = torch.zeros(len(list_of_nodes), dtype=torch.uint8)\n",
        "test_mask[- int(0.2 * len(list_of_nodes)):] = 1  # Use remaining 20% of nodes for testing.\n",
        "train_mask = train_mask.bool()  # Convert the training mask to boolean values.\n",
        "test_mask = test_mask.bool()    # Convert the testing mask to boolean values.\n",
        "\n",
        "#L Create a PyTorch Geometric data object. This object holds the graph data (features x, targets y, edge indices) and the masks for training and testing.\n",
        "data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, test_mask=test_mask)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXvFan9yM1I1"
      },
      "source": [
        "Case C: Create PyG dataset object using custom class and input files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-pJLIiLMvyo"
      },
      "source": [
        "class MyOwnDataset(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None, pre_transform=None):  #A\n",
        "        super(MyOwnDataset, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self): #B\n",
        "        return []\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self): #C\n",
        "        return ['../test.dataset']\n",
        "\n",
        "    def download(self): #D\n",
        "        # Download to `self.raw_dir`.\n",
        "        pass\n",
        "\n",
        "    def process(self): #E\n",
        "        # Read data into `Data` list.\n",
        "        data_list = []\n",
        "\n",
        "        eg = nx.read_edgelist('edge_list2.txt')\n",
        "\n",
        "        list_of_nodes = list(set(list(eg)))\n",
        "        indices_of_nodes = [list_of_nodes.index(x) for x in list_of_nodes]\n",
        "\n",
        "        node_to_index = dict(zip(list_of_nodes, indices_of_nodes))\n",
        "        index_to_node = dict(zip(indices_of_nodes, list_of_nodes))\n",
        "\n",
        "        list_edges = nx.convert.to_edgelist(eg)\n",
        "        list_edges = list(list_edges)\n",
        "        named_edge_list_0 = [x[0] for x in list_edges]\n",
        "        named_edge_list_1 = [x[1] for x in list_edges]\n",
        "\n",
        "        indexed_edge_list_0 = [node_to_index[x] for x in named_edge_list_0]\n",
        "        indexed_edge_list_1 = [node_to_index[x] for x in named_edge_list_1]\n",
        "\n",
        "        x = torch.FloatTensor([[1] for x in range(len(list_of_nodes))])#  [[] for x in xrange(n)]\n",
        "        y = torch.FloatTensor([1]*974 + [0]*973)\n",
        "        y = y.long()\n",
        "\n",
        "        edge_index = torch.tensor([indexed_edge_list_0, indexed_edge_list_1])\n",
        "\n",
        "        train_mask = torch.zeros(len(list_of_nodes), dtype=torch.uint8)\n",
        "        train_mask[:int(0.8 * len(list_of_nodes))] = 1 #train only on the 80% nodes\n",
        "        test_mask = torch.zeros(len(list_of_nodes), dtype=torch.uint8) #test on 20 % nodes\n",
        "        test_mask[- int(0.2 * len(list_of_nodes)):] = 1\n",
        "\n",
        "        train_mask = train_mask.bool()\n",
        "\n",
        "        test_mask = test_mask.bool()\n",
        "\n",
        "        data_example = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, test_mask=test_mask) #F\n",
        "\n",
        "        data_list.append(data_example) #G\n",
        "\n",
        "        data, slices = self.collate(data_list)  #H\n",
        "        torch.save((data, slices), self.processed_paths[0])  #I"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtSvMpANzufv"
      },
      "source": [
        "Case D: Create PyG data objects for use in dataloader without use of a dataset object\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTItD1H9NOBe"
      },
      "source": [
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# data_list = [Data(...), ..., Data(...)]\n",
        "# We'll use the data object from case B\n",
        "data_list = [data]\n",
        "loader = DataLoader(data_list, batch_size=32)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "We've covered the ETL and preprocessing steps in detail, providing insights into the handling and preparation of graph data for modeling with PyG. For the next set of chapter notebooks, we’ll be focusing on modeling and predictions.\n"
      ],
      "metadata": {
        "id": "b7Mp6BjoP5LY"
      }
    }
  ]
}