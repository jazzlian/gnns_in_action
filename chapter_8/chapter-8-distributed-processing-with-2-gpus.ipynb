{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Section 8.8 of GNNs in Action\n## Parallel & Distributed Processing\n\nThis notebook demonstrates the use of the datadistributedparallel class in training a GNN model on a machine with two GPUs. \n\nNOTE: You must activate the  GPU T4x2 accelerator to use this code.","metadata":{"_uuid":"fce707d7-9d00-4884-bcb7-6b7803a94010","_cell_guid":"6b66cd97-badd-405c-ad64-df71d9e6ba9b","trusted":true}},{"cell_type":"code","source":"!pip uninstall torch -y","metadata":{"_uuid":"316df8e1-3a6e-40dd-9ce2-8aee15e0cb88","_cell_guid":"45a6826f-c37a-4c7f-9295-d99a098083f5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:49:55.931983Z","iopub.execute_input":"2023-10-01T15:49:55.932724Z","iopub.status.idle":"2023-10-01T15:50:28.194120Z","shell.execute_reply.started":"2023-10-01T15:49:55.932691Z","shell.execute_reply":"2023-10-01T15:50:28.193078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch","metadata":{"_uuid":"d3e2c4d2-39ff-4e51-a320-65371371adb6","_cell_guid":"6ebfd462-c8d5-46c9-918f-3a88d19c77d8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:50:28.196367Z","iopub.execute_input":"2023-10-01T15:50:28.196730Z","iopub.status.idle":"2023-10-01T15:52:50.734729Z","shell.execute_reply.started":"2023-10-01T15:50:28.196698Z","shell.execute_reply":"2023-10-01T15:52:50.733481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('test')","metadata":{"_uuid":"ce53af6f-f780-4f75-b1a8-c26bbaab0ec2","_cell_guid":"99c8489e-6319-4e58-906a-787c5597d773","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:52:50.736722Z","iopub.execute_input":"2023-10-01T15:52:50.737295Z","iopub.status.idle":"2023-10-01T15:52:50.744609Z","shell.execute_reply.started":"2023-10-01T15:52:50.737257Z","shell.execute_reply":"2023-10-01T15:52:50.742017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the CUDA version PyTorch was installed with\n!python -c \"import torch; print(torch.version.cuda)\"","metadata":{"_uuid":"a4b1a3fc-7b7b-418c-be1b-1aeff219b11d","_cell_guid":"c1be376c-c7af-4231-85c2-6954d5a83139","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:52:50.749042Z","iopub.execute_input":"2023-10-01T15:52:50.749267Z","iopub.status.idle":"2023-10-01T15:52:54.450293Z","shell.execute_reply.started":"2023-10-01T15:52:50.749247Z","shell.execute_reply":"2023-10-01T15:52:54.449164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PyTorch version\n!python -c \"import torch; print(torch.__version__)\"","metadata":{"_uuid":"06328210-b6cf-4fba-9132-3998e349ef80","_cell_guid":"05af72f1-1785-4959-a1d5-245ffbfd8c6a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:52:54.452091Z","iopub.execute_input":"2023-10-01T15:52:54.452442Z","iopub.status.idle":"2023-10-01T15:52:57.245835Z","shell.execute_reply.started":"2023-10-01T15:52:54.452407Z","shell.execute_reply":"2023-10-01T15:52:57.244689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#########\n!pip install torch_geometric\n\n# Optional dependencies:\n!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.1+cu117.html","metadata":{"_uuid":"e167a01d-3d52-48a8-8df3-8b6a76742d41","_cell_guid":"4de72ae5-a0df-4d18-a341-3ab3297630bf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:52:57.247400Z","iopub.execute_input":"2023-10-01T15:52:57.247779Z","iopub.status.idle":"2023-10-01T15:53:29.208457Z","shell.execute_reply.started":"2023-10-01T15:52:57.247743Z","shell.execute_reply":"2023-10-01T15:53:29.207285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####\n!pip install ogb\n!pip install gputil\n!pip install nvidia-ml-py3\n!pip install thop","metadata":{"_uuid":"36e47e28-ab36-4789-aad9-3fe2da5f5a06","_cell_guid":"8b519504-91f2-4969-be9a-ba67256d7397","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T19:26:39.332952Z","iopub.execute_input":"2023-10-01T19:26:39.333313Z","iopub.status.idle":"2023-10-01T19:27:15.297172Z","shell.execute_reply.started":"2023-10-01T19:26:39.333285Z","shell.execute_reply":"2023-10-01T19:27:15.295826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####\n# import ogb\nfrom ogb.nodeproppred import PygNodePropPredDataset\ndataset = PygNodePropPredDataset(name='ogbn-products')\n# dataset = []","metadata":{"_uuid":"8eaff2f3-3ba5-4e58-a33e-1a28e03964f3","_cell_guid":"680d0430-31b7-4309-b286-e65227b44227","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:58:11.779658Z","iopub.execute_input":"2023-10-01T15:58:11.780005Z","iopub.status.idle":"2023-10-01T16:01:06.223905Z","shell.execute_reply.started":"2023-10-01T15:58:11.779978Z","shell.execute_reply":"2023-10-01T16:01:06.222762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encapsulate model and training into a file\nWe're writing the GCN model and training function into a separate Python file 'my_module.py'. This separation is essential for multiprocessing, particularly when using the multiprocessing library in PyTorch. By placing the model and training function in a separate file, we ensure that each process can import and access these components cleanly, preventing potential issues related to variable scope, function definitions, and Python's \"__main__\" guard during multiprocessing.\n\nAs for the script itself, this Python script is designed for distributed training of a Graph Convolutional Network (GCN) on a graph-based dataset using PyTorch and PyTorch Geometric. The code is divided into several parts, each serving a specific purpose. First, necessary libraries and modules are imported, including those required for distributed computing and memory usage tracking. The get_memory_usage function uses the psutil library to monitor the memory usage of the running process. The GCN class defines the architecture of the Graph Convolutional Network, including two GCN layers and a final fully connected layer for classification.\n\nThe train function is where the model is trained for one epoch. It calculates the loss, performs backpropagation, and updates the modelâ€™s weights. During each epoch, it monitors and prints the memory usage and epoch time. The main function sets up distributed training, where each process is assigned to a separate GPU. It initializes the model, data, and other training necessities and runs the training loop for a specified number of epochs. It calculates and prints the average epoch time, memory usage, and total convergence time for the training process.\n","metadata":{}},{"cell_type":"code","source":"%%writefile my_module.py\n\n# Importing necessary libraries\nimport torch\nfrom torch.nn import Linear\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.loader import NeighborLoader\nfrom ogb.nodeproppred import PygNodePropPredDataset\nimport time\nimport os\nimport psutil  # Library for retrieving system-level information, for our case, CPU memory usage\nimport GPUtil\nimport pynvml\nimport numpy as np  \nimport logging  \nfrom thop import profile  \n\n# Suppress INFO messages from thop\nlogger = logging.getLogger('thop')\nlogger.setLevel(logging.ERROR)\nlogging.getLogger('thop').setLevel(logging.WARNING)  \n\n# Initialize NVML library\npynvml.nvmlInit()\n\n# Function to get the current process's memory usage\ndef get_cpu_memory_usage():\n    \"\"\"\n    Returns the current memory usage of the cpu process.\n\n    :return: Memory usage in bytes\n    \"\"\"\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss\n\n\ndef get_gpu_memory_usage():\n    \"\"\"\n    Returns the current GPU memory usage using pynvml.\n\n    :return: GPU Memory usage in bytes\n    \"\"\"\n    handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # 0 for the first GPU\n    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n    return info.used  # This will return the used GPU memory in bytes\n\n\n\n# GCN (Graph Convolutional Network) model definition\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(100, 128)  # First GCN layer\n        self.conv2 = GCNConv(128, 128)  # Second GCN layer\n        self.fc = Linear(128, 47)  # Final fully connected layer, 47 classes for ogbn-products dataset\n\n    def forward(self, x, edge_index):\n        \"\"\"\n        Forward pass through the network.\n\n        :param x: Input features\n        :param edge_index: Edge indices defining the graph structure\n        :return: Output after passing through network\n        \"\"\"\n        x = self.conv1(x, edge_index)\n        x = self.conv2(x, edge_index)\n        x = self.fc(x)\n        return x\n\n# Function for training the model\ndef train(model, trainloader, criterion, optimizer, device, epoch):\n    model.train()\n    total_loss = 0.0\n    num_batches = 0\n\n    start_time = time.time()\n\n    memory_tracking = {\n        key: [] for key in [\n            'optimizer.zero_grad', 'model forward pass', 'target processing',\n            'loss calculation', 'loss.backward', 'optimizer.step'\n        ]\n    }\n\n    max_memory = 0  # Initialize the max_memory here\n    max_memory_step = \"\"\n    \n    total_flops = 0  # Initialize total FLOPs here\n\n    for batch in trainloader:\n        optimizer.zero_grad()\n        memory = get_gpu_memory_usage()\n        memory_tracking['optimizer.zero_grad'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'optimizer.zero_grad'\n\n        out = model(batch.x.float(), batch.edge_index)\n        memory = get_gpu_memory_usage()\n        memory_tracking['model forward pass'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'model forward pass'\n\n        target = batch.y.view(-1)\n        memory = get_gpu_memory_usage()\n        memory_tracking['target processing'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'target processing'\n\n        loss = criterion(out, target)\n        memory = get_gpu_memory_usage()\n        memory_tracking['loss calculation'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'loss calculation'\n\n        loss.backward()\n        memory = get_gpu_memory_usage()\n        memory_tracking['loss.backward'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'loss.backward'\n\n        optimizer.step()\n        memory = get_gpu_memory_usage()\n        memory_tracking['optimizer.step'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'optimizer.step'\n\n        # Calculating FLOPs\n        with torch.no_grad():\n            macs, params = profile(model.module, inputs=(batch.x.float().to(device), batch.edge_index.to(device)), verbose=False)\n            total_flops += macs * 2  # Convert MACs to FLOPs\n        \n        total_loss += loss.item()\n        num_batches += 1\n\n    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n\n    # Calculate and print memory stats\n    avg_memories = {k: np.mean(v) if v else 0 for k, v in memory_tracking.items()}\n    overall_avg_memory = np.mean([mem for mem in avg_memories.values() if mem > 0])\n\n    end_time = time.time()\n    epoch_time = end_time - start_time\n\n    print(f\"Device: {device}\\n\"\n      f\"Epoch: {epoch + 1}\\n\"\n      f\"Avg Loss: {avg_loss:.4f}\\n\"\n      f\"Avg Memory Usage: {overall_avg_memory/(1024 * 1024):.2f} MB\\n\"\n      f\"Max Memory Usage: {max_memory/(1024 * 1024):.2f} MB at {max_memory_step}\\n\"\n      f\"FLOPs: {(total_flops/num_batches if num_batches > 0 else 0):.2f}\\n\"\n      f\"Epoch Time: {epoch_time:.2f} s\\n\"\n      f\"{'='*40}\")\n\n    return epoch_time, overall_avg_memory, max_memory\n\n\n# Main function for distributed training\ndef main(rank, world_size, dataset):\n    \"\"\"\n    Main training loop for distributed training.\n\n    :param rank: Rank of the current process\n    :param world_size: Total number of processes\n    :param dataset: Dataset for training\n    \"\"\"\n    convergence_start_time = time.time()  # Time at the start of training\n    \n    split_idx = dataset.get_idx_split()\n    train_idx = split_idx[\"train\"]\n    \n    # Initializing distributed process group\n    dist.init_process_group(\n        backend='nccl',\n        init_method='tcp://localhost:23456',\n        rank=rank,\n        world_size=world_size\n    )\n    device = torch.device(f'cuda:{rank}')  # Set device for the current process\n    \n    data = dataset[0].to(device)  # Load data to the device\n\n    # Initializing DataLoader with neighbor sampling\n    trainloader = NeighborLoader(\n        data,\n        num_neighbors=[15, 10, 5],\n        batch_size=128 * 10,\n        input_nodes=train_idx,\n        shuffle=True,\n        persistent_workers=False\n    )\n  \n    model = GCN().to(device)\n    model = DistributedDataParallel(model, device_ids=[rank])  # Wrap model for distributed training\n\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Using Adam optimizer\n    \n    num_epochs = 2  # Total number of epochs\n    total_epoch_time = 0\n    total_memory_usage = 0\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        epoch_time, avg_memory_usage, max_memory_usage = train(model, trainloader, criterion, optimizer, device, epoch)\n    \n        total_epoch_time += epoch_time\n        total_memory_usage += avg_memory_usage\n    \n    # Calculating and printing average values and total convergence time\n    avg_epoch_time = total_epoch_time / num_epochs\n    avg_memory_usage = total_memory_usage / num_epochs * (1024 *1024)\n    convergence_time = time.time() - convergence_start_time\n    \n    print(f\"Average Epoch Time: {avg_epoch_time}s\")\n    print(f\"Average Memory Usage: {avg_memory_usage}MB\")\n    print(f\"Total Convergence Time: {convergence_time}s\")\n\n    pynvml.nvmlShutdown()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T20:03:50.124081Z","iopub.execute_input":"2023-10-01T20:03:50.124445Z","iopub.status.idle":"2023-10-01T20:03:50.134378Z","shell.execute_reply.started":"2023-10-01T20:03:50.124414Z","shell.execute_reply":"2023-10-01T20:03:50.133457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This script is responsible for initializing and executing the distributed training process. It begins by importing the main function from a file named \"my_module.py\". This main function contains the logic for training a GCN model, monitoring its performance, and tracking resource usage metrics like memory and time.\n\nThe script then imports PyTorch's multiprocessing module to facilitate the use of multiple processors, enabling parallelized operations and speeding up the training process.\n\nInside the if __name__ == '__main__': block, an initial print statement indicates the start of the preprocessing steps. The world_size variable is set to 2, implying that two separate processes will be spawned for training, each potentially utilizing a different GPU or set of computational resources.\n\nThe mp.spawn function is crucial here. It initiates the distributed training by creating world_size number of processes. Each process runs the main function with the provided arguments, performing the training concurrently and ensuring efficient utilization of available resources.\n\n","metadata":{}},{"cell_type":"code","source":"%%time  \n\n# Importing the 'main' function from 'my_module.py' which contains the \n# entire training process and resource tracking mechanisms.\nfrom my_module import main  \n\n# Importing the multiprocessing module from PyTorch, enabling the ability to \n# use multiple processes for parallel and distributed training.\nimport torch.multiprocessing as mp\n\n# Ensuring the main code is run under the __main__ scope to avoid potential\n# issues with multiprocessing across different modules.\nif __name__ == '__main__':\n    \n    # Setting the 'world_size' variable to 2, indicating that two processes \n    # will be spawned for distributed training.\n    # In the context of multi-GPU training, this typically means \n    # training will utilize two GPUs.\n    \n    world_size = 2  \n    \n    # The 'mp.spawn' function is used to spawn 'world_size' number of \n    # processes that will execute the 'main' function.\n    # Each process will run on a separate GPU (or other resources), \n    # enabling parallel and distributed training.\n    # The 'args' parameter is used to pass arguments to the 'main' \n    # function in each process.\n    # 'nprocs' is set to 'world_size', ensuring the number of processes \n    # spawned equals the defined 'world_size'.\n    # 'join=True' means the main process will wait for all spawned \n    # processes to complete before proceeding.\n    \n    mp.spawn(main, args=(world_size, dataset,), nprocs=world_size, join=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T20:03:50.896119Z","iopub.execute_input":"2023-10-01T20:03:50.897145Z","iopub.status.idle":"2023-10-01T20:06:51.569139Z","shell.execute_reply.started":"2023-10-01T20:03:50.897107Z","shell.execute_reply":"2023-10-01T20:06:51.568129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}