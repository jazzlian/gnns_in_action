{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Section 8.8 of GNNs in Action\n## Parallel & Distributed Processing\n\nThis notebook demonstrates the use of the datadistributedparallel class in training a GNN model on a machine with two GPUs. \n\nNOTE: You must activate the  GPU T4x2 accelerator to use this code.","metadata":{"_uuid":"fce707d7-9d00-4884-bcb7-6b7803a94010","_cell_guid":"6b66cd97-badd-405c-ad64-df71d9e6ba9b","trusted":true}},{"cell_type":"code","source":"!pip uninstall torch -y","metadata":{"_uuid":"316df8e1-3a6e-40dd-9ce2-8aee15e0cb88","_cell_guid":"45a6826f-c37a-4c7f-9295-d99a098083f5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:49:55.931983Z","iopub.execute_input":"2023-10-01T15:49:55.932724Z","iopub.status.idle":"2023-10-01T15:50:28.194120Z","shell.execute_reply.started":"2023-10-01T15:49:55.932691Z","shell.execute_reply":"2023-10-01T15:50:28.193078Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.0.0\nUninstalling torch-2.0.0:\n  Successfully uninstalled torch-2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install torch","metadata":{"_uuid":"d3e2c4d2-39ff-4e51-a320-65371371adb6","_cell_guid":"6ebfd462-c8d5-46c9-918f-3a88d19c77d8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:50:28.196367Z","iopub.execute_input":"2023-10-01T15:50:28.196730Z","iopub.status.idle":"2023-10-01T15:52:50.734729Z","shell.execute_reply.started":"2023-10-01T15:50:28.196698Z","shell.execute_reply":"2023-10-01T15:52:50.733481Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torch\n  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch)\n  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch)\n  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch)\n  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch)\n  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch)\n  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch)\n  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==2.0.0 (from torch)\n  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (68.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\nCollecting cmake (from triton==2.0.0->torch)\n  Downloading cmake-3.27.6-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lit (from triton==2.0.0->torch)\n  Downloading lit-17.0.1.tar.gz (154 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nBuilding wheels for collected packages: lit\n  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-17.0.1-py3-none-any.whl size=93254 sha256=f0c3bc1050afdd749e5cce8a432d7e95524502263df7e6da59b21a4eaee1abdd\n  Stored in directory: /root/.cache/pip/wheels/cf/3a/a0/f65551951357f983270eb3b210b98c6be543f3ed5cf89deba4\nSuccessfully built lit\nInstalling collected packages: lit, cmake, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchdata 0.6.0 requires torch==2.0.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cmake-3.27.6 lit-17.0.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"print('test')","metadata":{"_uuid":"ce53af6f-f780-4f75-b1a8-c26bbaab0ec2","_cell_guid":"99c8489e-6319-4e58-906a-787c5597d773","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:52:50.736722Z","iopub.execute_input":"2023-10-01T15:52:50.737295Z","iopub.status.idle":"2023-10-01T15:52:50.744609Z","shell.execute_reply.started":"2023-10-01T15:52:50.737257Z","shell.execute_reply":"2023-10-01T15:52:50.742017Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"test\n","output_type":"stream"}]},{"cell_type":"code","source":"# Find the CUDA version PyTorch was installed with\n!python -c \"import torch; print(torch.version.cuda)\"","metadata":{"_uuid":"a4b1a3fc-7b7b-418c-be1b-1aeff219b11d","_cell_guid":"c1be376c-c7af-4231-85c2-6954d5a83139","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:52:50.749042Z","iopub.execute_input":"2023-10-01T15:52:50.749267Z","iopub.status.idle":"2023-10-01T15:52:54.450293Z","shell.execute_reply.started":"2023-10-01T15:52:50.749247Z","shell.execute_reply":"2023-10-01T15:52:54.449164Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"11.7\n","output_type":"stream"}]},{"cell_type":"code","source":"# PyTorch version\n!python -c \"import torch; print(torch.__version__)\"","metadata":{"_uuid":"06328210-b6cf-4fba-9132-3998e349ef80","_cell_guid":"05af72f1-1785-4959-a1d5-245ffbfd8c6a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:52:54.452091Z","iopub.execute_input":"2023-10-01T15:52:54.452442Z","iopub.status.idle":"2023-10-01T15:52:57.245835Z","shell.execute_reply.started":"2023-10-01T15:52:54.452407Z","shell.execute_reply":"2023-10-01T15:52:57.244689Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"2.0.1+cu117\n","output_type":"stream"}]},{"cell_type":"code","source":"#########\n!pip install torch_geometric\n\n# Optional dependencies:\n!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.1+cu117.html","metadata":{"_uuid":"e167a01d-3d52-48a8-8df3-8b6a76742d41","_cell_guid":"4de72ae5-a0df-4d18-a341-3ab3297630bf","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:52:57.247400Z","iopub.execute_input":"2023-10-01T15:52:57.247779Z","iopub.status.idle":"2023-10-01T15:53:29.208457Z","shell.execute_reply.started":"2023-10-01T15:52:57.247743Z","shell.execute_reply":"2023-10-01T15:53:29.207285Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.23.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.11.2)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.31.0)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2023.7.22)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.1.0)\nBuilding wheels for collected packages: torch_geometric\n  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=be5cede34b4004a08ad7a3e80ee947b143c4db17a8937f823f02724a2ab6c0cb\n  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\nSuccessfully built torch_geometric\nInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.3.1\nLooking in links: https://data.pyg.org/whl/torch-2.0.1+cu117.html\nCollecting pyg_lib\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu117/pyg_lib-0.2.0%2Bpt20cu117-cp310-cp310-linux_x86_64.whl (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torch_scatter\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_scatter-2.1.1%2Bpt20cu117-cp310-cp310-linux_x86_64.whl (10.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hCollecting torch_sparse\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_sparse-0.6.17%2Bpt20cu117-cp310-cp310-linux_x86_64.whl (4.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torch_cluster\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_cluster-1.6.1%2Bpt20cu117-cp310-cp310-linux_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torch_spline_conv\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu117/torch_spline_conv-1.2.2%2Bpt20cu117-cp310-cp310-linux_x86_64.whl (885 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m885.5/885.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_sparse) (1.11.2)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy->torch_sparse) (1.23.5)\nInstalling collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\nSuccessfully installed pyg_lib-0.2.0+pt20cu117 torch_cluster-1.6.1+pt20cu117 torch_scatter-2.1.1+pt20cu117 torch_sparse-0.6.17+pt20cu117 torch_spline_conv-1.2.2+pt20cu117\n","output_type":"stream"}]},{"cell_type":"code","source":"####\n!pip install ogb\n!pip install gputil\n!pip install nvidia-ml-py3\n!pip install thop","metadata":{"_uuid":"36e47e28-ab36-4789-aad9-3fe2da5f5a06","_cell_guid":"8b519504-91f2-4969-be9a-ba67256d7397","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T19:26:39.332952Z","iopub.execute_input":"2023-10-01T19:26:39.333313Z","iopub.status.idle":"2023-10-01T19:27:15.297172Z","shell.execute_reply.started":"2023-10-01T19:26:39.333285Z","shell.execute_reply":"2023-10-01T19:27:15.295826Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Requirement already satisfied: ogb in /opt/conda/lib/python3.10/site-packages (1.3.6)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (2.0.1)\nRequirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.23.5)\nRequirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (4.66.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.2.2)\nRequirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (2.0.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.16.0)\nRequirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (1.26.15)\nRequirement already satisfied: outdated>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ogb) (0.2.2)\nRequirement already satisfied: setuptools>=44 in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (68.0.0)\nRequirement already satisfied: littleutils in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (0.2.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from outdated>=0.2.0->ogb) (2.31.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.24.0->ogb) (2023.3)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (1.11.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (11.7.99)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (11.7.99)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (11.7.101)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (11.10.3.66)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (10.2.10.91)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (11.4.0.1)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (11.7.4.91)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (2.14.3)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (11.7.91)\nRequirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->ogb) (2.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->ogb) (0.40.0)\nRequirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.27.6)\nRequirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.6.0->ogb) (17.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->ogb) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->outdated>=0.2.0->ogb) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\nRequirement already satisfied: gputil in /opt/conda/lib/python3.10/site-packages (1.4.0)\nRequirement already satisfied: nvidia-ml-py3 in /opt/conda/lib/python3.10/site-packages (7.352.0)\nCollecting thop\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from thop) (2.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->thop) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->thop) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (3.1.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (11.7.99)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (11.7.99)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (11.7.101)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (8.5.0.96)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (11.10.3.66)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (10.2.10.91)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (11.4.0.1)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (11.7.4.91)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (2.14.3)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (11.7.91)\nRequirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch->thop) (2.0.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->thop) (68.0.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->thop) (0.40.0)\nRequirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch->thop) (3.27.6)\nRequirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch->thop) (17.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->thop) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->thop) (1.3.0)\nInstalling collected packages: thop\nSuccessfully installed thop-0.1.1.post2209072238\n","output_type":"stream"}]},{"cell_type":"code","source":"####\n# import ogb\nfrom ogb.nodeproppred import PygNodePropPredDataset\ndataset = PygNodePropPredDataset(name='ogbn-products')\n# dataset = []","metadata":{"_uuid":"8eaff2f3-3ba5-4e58-a33e-1a28e03964f3","_cell_guid":"680d0430-31b7-4309-b286-e65227b44227","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-10-01T15:58:11.779658Z","iopub.execute_input":"2023-10-01T15:58:11.780005Z","iopub.status.idle":"2023-10-01T16:01:06.223905Z","shell.execute_reply.started":"2023-10-01T15:58:11.779978Z","shell.execute_reply":"2023-10-01T16:01:06.222762Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"This will download 1.38GB. Will you proceed? (y/N)\n y\n"},{"name":"stdout","text":"Downloading http://snap.stanford.edu/ogb/data/nodeproppred/products.zip\n","output_type":"stream"},{"name":"stderr","text":"Downloaded 1.38 GB: 100%|██████████| 1414/1414 [00:58<00:00, 24.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting dataset/products.zip\n","output_type":"stream"},{"name":"stderr","text":"Processing...\n","output_type":"stream"},{"name":"stdout","text":"Loading necessary files...\nThis might take a while.\nProcessing graphs...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n","output_type":"stream"},{"name":"stdout","text":"Converting graphs into PyG objects...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 1082.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Saving...\n","output_type":"stream"},{"name":"stderr","text":"Done!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Encapsulate model and training into a file\nWe're writing the GCN model and training function into a separate Python file 'my_module.py'. This separation is essential for multiprocessing, particularly when using the multiprocessing library in PyTorch. By placing the model and training function in a separate file, we ensure that each process can import and access these components cleanly, preventing potential issues related to variable scope, function definitions, and Python's \"__main__\" guard during multiprocessing.\n\nAs for the script itself, this Python script is designed for distributed training of a Graph Convolutional Network (GCN) on a graph-based dataset using PyTorch and PyTorch Geometric. The code is divided into several parts, each serving a specific purpose. First, necessary libraries and modules are imported, including those required for distributed computing and memory usage tracking. The get_memory_usage function uses the psutil library to monitor the memory usage of the running process. The GCN class defines the architecture of the Graph Convolutional Network, including two GCN layers and a final fully connected layer for classification.\n\nThe train function is where the model is trained for one epoch. It calculates the loss, performs backpropagation, and updates the model’s weights. During each epoch, it monitors and prints the memory usage and epoch time. The main function sets up distributed training, where each process is assigned to a separate GPU. It initializes the model, data, and other training necessities and runs the training loop for a specified number of epochs. It calculates and prints the average epoch time, memory usage, and total convergence time for the training process.\n","metadata":{}},{"cell_type":"code","source":"%%writefile my_module.py\n\n# Importing necessary libraries\nimport torch\nfrom torch.nn import Linear\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.loader import NeighborLoader\nfrom ogb.nodeproppred import PygNodePropPredDataset\nimport time\nimport os\nimport psutil  # Library for retrieving system-level information, for our case, CPU memory usage\nimport GPUtil\nimport pynvml\nimport numpy as np  \nimport logging  \nfrom thop import profile  \n\n# Suppress INFO messages from thop\nlogger = logging.getLogger('thop')\nlogger.setLevel(logging.ERROR)\nlogging.getLogger('thop').setLevel(logging.WARNING)  \n\n# Initialize NVML library\npynvml.nvmlInit()\n\n# Function to get the current process's memory usage\ndef get_cpu_memory_usage():\n    \"\"\"\n    Returns the current memory usage of the cpu process.\n\n    :return: Memory usage in bytes\n    \"\"\"\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss\n\n\ndef get_gpu_memory_usage():\n    \"\"\"\n    Returns the current GPU memory usage using pynvml.\n\n    :return: GPU Memory usage in bytes\n    \"\"\"\n    handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # 0 for the first GPU\n    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n    return info.used  # This will return the used GPU memory in bytes\n\n\n\n# GCN (Graph Convolutional Network) model definition\nclass GCN(torch.nn.Module):\n    def __init__(self):\n        super(GCN, self).__init__()\n        self.conv1 = GCNConv(100, 128)  # First GCN layer\n        self.conv2 = GCNConv(128, 128)  # Second GCN layer\n        self.fc = Linear(128, 47)  # Final fully connected layer, 47 classes for ogbn-products dataset\n\n    def forward(self, x, edge_index):\n        \"\"\"\n        Forward pass through the network.\n\n        :param x: Input features\n        :param edge_index: Edge indices defining the graph structure\n        :return: Output after passing through network\n        \"\"\"\n        x = self.conv1(x, edge_index)\n        x = self.conv2(x, edge_index)\n        x = self.fc(x)\n        return x\n\n# Function for training the model\ndef train(model, trainloader, criterion, optimizer, device, epoch):\n    model.train()\n    total_loss = 0.0\n    num_batches = 0\n\n    start_time = time.time()\n\n    memory_tracking = {\n        key: [] for key in [\n            'optimizer.zero_grad', 'model forward pass', 'target processing',\n            'loss calculation', 'loss.backward', 'optimizer.step'\n        ]\n    }\n\n    max_memory = 0  # Initialize the max_memory here\n    max_memory_step = \"\"\n    \n    total_flops = 0  # Initialize total FLOPs here\n\n    for batch in trainloader:\n        optimizer.zero_grad()\n        memory = get_gpu_memory_usage()\n        memory_tracking['optimizer.zero_grad'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'optimizer.zero_grad'\n\n        out = model(batch.x.float(), batch.edge_index)\n        memory = get_gpu_memory_usage()\n        memory_tracking['model forward pass'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'model forward pass'\n\n        target = batch.y.view(-1)\n        memory = get_gpu_memory_usage()\n        memory_tracking['target processing'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'target processing'\n\n        loss = criterion(out, target)\n        memory = get_gpu_memory_usage()\n        memory_tracking['loss calculation'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'loss calculation'\n\n        loss.backward()\n        memory = get_gpu_memory_usage()\n        memory_tracking['loss.backward'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'loss.backward'\n\n        optimizer.step()\n        memory = get_gpu_memory_usage()\n        memory_tracking['optimizer.step'].append(memory)\n        if memory > max_memory:  # Update max_memory if the current memory is greater\n            max_memory = memory\n            max_memory_step = 'optimizer.step'\n\n        # Calculating FLOPs\n        with torch.no_grad():\n            macs, params = profile(model.module, inputs=(batch.x.float().to(device), batch.edge_index.to(device)), verbose=False)\n            total_flops += macs * 2  # Convert MACs to FLOPs\n        \n        total_loss += loss.item()\n        num_batches += 1\n\n    avg_loss = total_loss / num_batches if num_batches > 0 else 0\n\n    # Calculate and print memory stats\n    avg_memories = {k: np.mean(v) if v else 0 for k, v in memory_tracking.items()}\n    overall_avg_memory = np.mean([mem for mem in avg_memories.values() if mem > 0])\n\n    end_time = time.time()\n    epoch_time = end_time - start_time\n\n    # Printing epoch time, loss, memory, and FLOPs stats\n#     print(f\"Device: {device}, Epoch: {epoch + 1}, Avg Loss: {avg_loss}, \"\n#           f\"Avg Memory Usage: {overall_avg_memory/(1024 * 1024)}MB, \"\n#           f\"Max Memory Usage: {max_memory/(1024 * 1024)}MB at {max_memory_step}, \"\n#           f\"FLOPs: {total_flops/num_batches if num_batches > 0 else 0}, \"  # Printing average FLOPs per batch\n#           f\"Epoch Time: {epoch_time}s\\n\")\n    print(f\"Device: {device}\\n\"\n      f\"Epoch: {epoch + 1}\\n\"\n      f\"Avg Loss: {avg_loss:.4f}\\n\"\n      f\"Avg Memory Usage: {overall_avg_memory/(1024 * 1024):.2f} MB\\n\"\n      f\"Max Memory Usage: {max_memory/(1024 * 1024):.2f} MB at {max_memory_step}\\n\"\n      f\"FLOPs: {(total_flops/num_batches if num_batches > 0 else 0):.2f}\\n\"\n      f\"Epoch Time: {epoch_time:.2f} s\\n\"\n      f\"{'='*40}\")\n\n    return epoch_time, overall_avg_memory, max_memory\n\n\n# Main function for distributed training\ndef main(rank, world_size, dataset):\n    \"\"\"\n    Main training loop for distributed training.\n\n    :param rank: Rank of the current process\n    :param world_size: Total number of processes\n    :param dataset: Dataset for training\n    \"\"\"\n    convergence_start_time = time.time()  # Time at the start of training\n    \n    split_idx = dataset.get_idx_split()\n    train_idx = split_idx[\"train\"]\n    \n    # Initializing distributed process group\n    dist.init_process_group(\n        backend='nccl',\n        init_method='tcp://localhost:23456',\n        rank=rank,\n        world_size=world_size\n    )\n    device = torch.device(f'cuda:{rank}')  # Set device for the current process\n    \n    data = dataset[0].to(device)  # Load data to the device\n\n    # Initializing DataLoader with neighbor sampling\n    trainloader = NeighborLoader(\n        data,\n        num_neighbors=[15, 10, 5],\n        batch_size=128 * 10,\n        input_nodes=train_idx,\n        shuffle=True,\n        persistent_workers=False\n    )\n  \n    model = GCN().to(device)\n    model = DistributedDataParallel(model, device_ids=[rank])  # Wrap model for distributed training\n\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Using Adam optimizer\n    \n    num_epochs = 2  # Total number of epochs\n    total_epoch_time = 0\n    total_memory_usage = 0\n    \n    # Training loop\n    for epoch in range(num_epochs):\n        epoch_time, avg_memory_usage, max_memory_usage = train(model, trainloader, criterion, optimizer, device, epoch)\n    \n        total_epoch_time += epoch_time\n        total_memory_usage += avg_memory_usage\n    \n    # Calculating and printing average values and total convergence time\n    avg_epoch_time = total_epoch_time / num_epochs\n    avg_memory_usage = total_memory_usage / num_epochs * (1024 *1024)\n    convergence_time = time.time() - convergence_start_time\n    \n    print(f\"Average Epoch Time: {avg_epoch_time}s\")\n    print(f\"Average Memory Usage: {avg_memory_usage}MB\")\n    print(f\"Total Convergence Time: {convergence_time}s\")\n\n    pynvml.nvmlShutdown()","metadata":{"execution":{"iopub.status.busy":"2023-10-01T20:03:50.124081Z","iopub.execute_input":"2023-10-01T20:03:50.124445Z","iopub.status.idle":"2023-10-01T20:03:50.134378Z","shell.execute_reply.started":"2023-10-01T20:03:50.124414Z","shell.execute_reply":"2023-10-01T20:03:50.133457Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"Overwriting my_module.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This script is responsible for initializing and executing the distributed training process. It begins by importing the main function from a file named \"my_module.py\". This main function contains the logic for training a GCN model, monitoring its performance, and tracking resource usage metrics like memory and time.\n\nThe script then imports PyTorch's multiprocessing module to facilitate the use of multiple processors, enabling parallelized operations and speeding up the training process.\n\nInside the if __name__ == '__main__': block, an initial print statement indicates the start of the preprocessing steps. The world_size variable is set to 2, implying that two separate processes will be spawned for training, each potentially utilizing a different GPU or set of computational resources.\n\nThe mp.spawn function is crucial here. It initiates the distributed training by creating world_size number of processes. Each process runs the main function with the provided arguments, performing the training concurrently and ensuring efficient utilization of available resources.\n\n","metadata":{}},{"cell_type":"code","source":"%%time  \n\n# Importing the 'main' function from 'my_module.py' which contains the \n# entire training process and resource tracking mechanisms.\nfrom my_module import main  \n\n# Importing the multiprocessing module from PyTorch, enabling the ability to \n# use multiple processes for parallel and distributed training.\nimport torch.multiprocessing as mp\n\n# Ensuring the main code is run under the __main__ scope to avoid potential\n# issues with multiprocessing across different modules.\nif __name__ == '__main__':\n    \n    # Setting the 'world_size' variable to 2, indicating that two processes \n    # will be spawned for distributed training.\n    # In the context of multi-GPU training, this typically means \n    # training will utilize two GPUs.\n    \n    world_size = 2  \n    \n    # The 'mp.spawn' function is used to spawn 'world_size' number of \n    # processes that will execute the 'main' function.\n    # Each process will run on a separate GPU (or other resources), \n    # enabling parallel and distributed training.\n    # The 'args' parameter is used to pass arguments to the 'main' \n    # function in each process.\n    # 'nprocs' is set to 'world_size', ensuring the number of processes \n    # spawned equals the defined 'world_size'.\n    # 'join=True' means the main process will wait for all spawned \n    # processes to complete before proceeding.\n    \n    mp.spawn(main, args=(world_size, dataset,), nprocs=world_size, join=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T20:03:50.896119Z","iopub.execute_input":"2023-10-01T20:03:50.897145Z","iopub.status.idle":"2023-10-01T20:06:51.569139Z","shell.execute_reply.started":"2023-10-01T20:03:50.897107Z","shell.execute_reply":"2023-10-01T20:06:51.568129Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n[W socket.cpp:601] [c10d] The client socket has failed to connect to [localhost]:23456 (errno: 99 - Cannot assign requested address).\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda:1\nEpoch: 1\nAvg Loss: 1.5268\nAvg Memory Usage: 8872.76 MB\nMax Memory Usage: 8873.00 MB at model forward pass\nFLOPs: 5690671517.92\nEpoch Time: 81.21 s\n========================================\nDevice: cuda:1\nEpoch: 2\nAvg Loss: 1.4496\nAvg Memory Usage: 8873.00 MB\nMax Memory Usage: 8873.00 MB at optimizer.zero_grad\nFLOPs: 5687735397.40\nEpoch Time: 79.93 s\n========================================\nAverage Epoch Time: 80.56825232505798s\nAverage Memory Usage: 9755834589067396.0MB\nTotal Convergence Time: 171.85723757743835s\nDevice: cuda:0\nEpoch: 1\nAvg Loss: 1.5284\nAvg Memory Usage: 8872.73 MB\nMax Memory Usage: 8873.00 MB at loss.backward\nFLOPs: 5691030602.81\nEpoch Time: 81.20 s\n========================================\nDevice: cuda:0\nEpoch: 2\nAvg Loss: 1.4501\nAvg Memory Usage: 8873.00 MB\nMax Memory Usage: 8873.00 MB at optimizer.zero_grad\nFLOPs: 5690648078.96\nEpoch Time: 79.93 s\n========================================\nAverage Epoch Time: 80.56910145282745s\nAverage Memory Usage: 9755819119747958.0MB\nTotal Convergence Time: 171.84109950065613s\nCPU times: user 12 ms, sys: 6.03 ms, total: 18 ms\nWall time: 3min\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}